[{"text": "by HG's (HL). In particular, we show that HL's are included in TAL's andthat TAG's are equivalent toa modification ofHG:s  called Modified Head Grammars (MHG's). The inclusion of MHL in HI.,,", "acronyms": [[154, 159], [3, 5], [9, 11], [42, 44], [63, 66], [77, 80], [179, 182], [117, 119]], "long-forms": [[130, 152]]}, {"text": "in cooperation between HU Berlin, U Frankfurt and U Jena, conducted in the wider context of the Deutsch Diachron Digital (DDD) initiative. The", "acronyms": [[122, 125], [23, 25]], "long-forms": [[96, 120]]}, {"text": "Abstract  This paper presents a new bootstrapping  approach to named entity (NE)  classification.", "acronyms": [[77, 79]], "long-forms": [[63, 75]]}, {"text": " Table 5: EPPS task: translation quality and time for different input conditions (CN=confusion network, time in seconds per sentence).", "acronyms": [[82, 84], [10, 14]], "long-forms": [[85, 102]]}, {"text": "He poured wine from the barrel into the bottle The semantic description of (20) appeals to an intermediate locus IME(LOC), which is not specified here.", "acronyms": [[117, 120], [113, 116]], "long-forms": [[107, 112]]}, {"text": " ? Fondazione Bruno Kessler (FBK-irst), Italy ?", "acronyms": [[29, 37]], "long-forms": [[3, 27]]}, {"text": "been semi-automatically detected in human reference and machine translations from English (EN) to French (FR) and German (DE) (Section 3). ", "acronyms": [[106, 108], [91, 93], [122, 124]], "long-forms": [[98, 104], [82, 89], [114, 120]]}, {"text": "In Proceedings of the NAACL/AMTA Workshop on Syntax and Structure in Statistical Translation (SSST), pages 33?40, Rochester, NY.", "acronyms": [[94, 98], [22, 32], [125, 127]], "long-forms": [[45, 92]]}, {"text": "crimination information (MDI) estimation criterion for speech recognition and notes an improvement in terms of perplexity and word error rate (WER). ", "acronyms": [[143, 146], [25, 29]], "long-forms": [[126, 141]]}, {"text": "2004. Evaluation of a Deidentification (De-Id) Software Engine to Share Pathology Reports and Clinical Documents for", "acronyms": [[40, 45]], "long-forms": [[22, 38]]}, {"text": "We then review some standard online learners (e.g. perceptron) before presenting the Bayes Point Machine (BPM) (Herbrich et al, 2001; Harrington et al, 2003).", "acronyms": [[106, 109]], "long-forms": [[85, 104]]}, {"text": "grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) Figure 3: Plots of imageability scores for literal vs. nonliteral/metaphorical words in the VUAMC, grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) clearest distinction between literal vs. nonliteral items.", "acronyms": [[223, 225], [39, 41], [177, 182]], "long-forms": [[226, 236], [30, 37], [42, 52], [56, 60], [64, 68], [72, 83], [214, 221], [240, 244], [248, 252], [256, 267]]}, {"text": "the parameters ? = (s,W,b,x) via backpropagation with stochastic gradient descent (SGD). ", "acronyms": [[83, 86]], "long-forms": [[54, 81]]}, {"text": "forward and backward application (FA and BA), implies a  standard notion of constituency, rules like type raising (TR)  and functional composition (FC) give rise to a more  generous notion of constituency (this is what makes 'non- ", "acronyms": [[148, 150]], "long-forms": [[124, 146]]}, {"text": "196  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1779?1785, October 25-29, 2014, Doha, Qatar.", "acronyms": [[93, 98]], "long-forms": [[43, 91]]}, {"text": "composition algorithms have been proposed in the meantime. Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version", "acronyms": [[90, 93]], "long-forms": [[65, 88]]}, {"text": " 207  Informational Contribution (IC) function for each element  in a pair.", "acronyms": [[34, 36]], "long-forms": [[6, 32]]}, {"text": "three statistical models: Conditional Random  Fields(CRF), Maximum Entropy(ME), and  Support Vector Machine(SVM), which have  good performance and used widely in the ", "acronyms": [[108, 111], [53, 56], [75, 77]], "long-forms": [[85, 106], [26, 52], [59, 74]]}, {"text": "lingual interpersonal communication. The improvements in automatic speech recognition (ASR), statistical machine translation (MT), and, text-to-speech", "acronyms": [[87, 90], [126, 128]], "long-forms": [[57, 85], [105, 124]]}, {"text": "PROJECT GOALS  This project involves the integration of speech and natural-  language processing for spoken language systems (SLS). The ", "acronyms": [[126, 129]], "long-forms": [[101, 124]]}, {"text": " won out on F-measure while giza++ syllabized attained better alignment error rate (AER). Refer to Table 3 for", "acronyms": [[84, 87]], "long-forms": [[62, 82]]}, {"text": "The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). Language models", "acronyms": [[100, 103], [58, 61]], "long-forms": [[73, 98]]}, {"text": "plains the text? Our approach is related to minimum description length (MDL). We formulate our", "acronyms": [[72, 75]], "long-forms": [[48, 70]]}, {"text": "Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.", "acronyms": [[124, 128], [26, 31], [73, 77]], "long-forms": [[93, 122], [0, 24], [33, 71]]}, {"text": "fn-n1?. The annotation contains a feature structure with three features: FE (Frame element), GF (Grammatical Function), and PT", "acronyms": [[73, 75], [93, 95], [124, 126]], "long-forms": [[77, 90], [97, 117]]}, {"text": "1Note: LM(language model); ME(maximum entropy).  Brand Name(BRA), Product Type(TYP), Product Name(PRO), and BRA and TYP are often embed-", "acronyms": [[60, 63], [7, 9], [27, 29], [79, 82], [98, 101], [108, 111], [116, 119]], "long-forms": [[49, 54], [10, 24], [30, 45], [66, 78], [85, 92]]}, {"text": "  NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 1?4, Montre?al, Canada, June 7?8, 2012.", "acronyms": [[88, 93], [2, 11]], "long-forms": [[29, 86]]}, {"text": "(Bjo?rne et al 2011).  The Turku Event Extraction System (TEES)1 is an open source program for extracting events and re-", "acronyms": [[58, 62]], "long-forms": [[27, 56]]}, {"text": "(Person Names) by prometheus e. V.16, and Getty ULAN (United List of Artist Names)17 There are two modes of use for name authorities", "acronyms": [[48, 52]], "long-forms": [[54, 84]]}, {"text": "lation of term-lists, related studies are found in the  area of target word selection (for content words) in  conventional full-text machine translation (MT). ", "acronyms": [[154, 156]], "long-forms": [[133, 152]]}, {"text": "weiwei@cs.columbia.edu Abstract In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence,", "acronyms": [[64, 67]], "long-forms": [[35, 62]]}, {"text": "To experiment on MAYA, we compute the  performance score as the Reciprocal Answer  Rank (RAR) of the first correct answer given by  each question.", "acronyms": [[89, 92], [17, 21]], "long-forms": [[83, 87]]}, {"text": "Because of a scarcity of such corpora, most work has used the International Corpus of Learner English (ICLEv2) (Granger et al 2009) for training and evaluation", "acronyms": [[103, 109]], "long-forms": [[62, 101]]}, {"text": "read speech and two-party dialogue, multi-party dialogues typically exhibit a considerably higher word error rate (WER) (Morgan et al, 2003). ", "acronyms": [[115, 118]], "long-forms": [[98, 113]]}, {"text": "Two categories of neural networks have been used for language modeling: 1) recurrent neural networks (RNN), and 2) feedfoward network (FFN):", "acronyms": [[102, 105], [135, 138]], "long-forms": [[75, 100], [115, 133]]}, {"text": "cal Dirichlet Process (HDP) (Teh et al, 2006), a Bayesian nonparametric variant of Latent Dirichlet Allocation (LDA), to automatically infer the number of topics.", "acronyms": [[112, 115], [23, 26]], "long-forms": [[83, 110], [0, 21]]}, {"text": "ily' (lists truncated). Score = log-likelihood score; f = occurrence frequency of keyterm; NN = noun; VV = verb; AR =  article; AP = article+preposition; JJ = adjective; CC = con-", "acronyms": [[91, 93], [24, 29], [102, 104], [113, 115], [128, 130], [154, 156], [170, 172]], "long-forms": [[96, 100], [69, 78], [47, 52], [107, 111], [119, 126], [133, 152], [159, 168]]}, {"text": "? ? ? ?  Figure 2: Laten Event Model (LEM). ", "acronyms": [[38, 41]], "long-forms": [[19, 36]]}, {"text": "F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which does not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-", "acronyms": [[154, 157]], "long-forms": [[132, 152]]}, {"text": "icoglu and Bergler, 2009). The second group uses a  machine learning (ML)-based approach which exploits various specific features and learning algo-", "acronyms": [[70, 72]], "long-forms": [[52, 68]]}, {"text": "the noun phrase (NP) rule, a top-down parser is delaying making any commitments about the category following the determiner (DT). This delay in predic-", "acronyms": [[125, 127], [17, 19]], "long-forms": [[113, 123], [4, 15]]}, {"text": "and GL.  GL = GR or GL unspec. CC", "acronyms": [[14, 16], [4, 6], [9, 11], [31, 33]], "long-forms": [[17, 19]]}, {"text": "tions. This model has three main steps including Local Term Weighting (LTW), Global Term Weighting (GTM), and Fuzzy Clustering (Algo-", "acronyms": [[71, 74]], "long-forms": [[49, 69]]}, {"text": "{FirstName.SecondName}@dfki.de Abstract The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system", "acronyms": [[121, 123], [44, 48]], "long-forms": [[97, 119]]}, {"text": "Proc. of the IEEE International Conference on Data Mining (ICDM). ", "acronyms": [[59, 63]], "long-forms": [[54, 57]]}, {"text": "Headline Generation. In the Proceedings of the  Document Understanding Conference (DUC). ", "acronyms": [[83, 86]], "long-forms": [[48, 81]]}, {"text": "Machine Translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 192?202, Sapporo, Japan.", "acronyms": [[94, 97]], "long-forms": [[51, 92]]}, {"text": "   grandmother. CL.1SG.GEN ALL ART=airport  my grandmother to the airport ", "acronyms": [[31, 34], [16, 26]], "long-forms": [[35, 42]]}, {"text": "to the recent evaluations of domain-independent Q/A systems organized in the context of the Text REtrieval Conference (TREC)1. The TREC", "acronyms": [[119, 123], [48, 51], [131, 135]], "long-forms": [[92, 117]]}, {"text": "Hong Kong Laws, Sinorama, Xinhua News, and English translation of Chinese Treebank), available from the Linguistic Data Consortium (LDC). To", "acronyms": [[132, 135]], "long-forms": [[104, 130]]}, {"text": "text refer to the same entity in real world or not.  Noun Phrase CRR (NP-CRR) considers all noun  phrases as entities, while Named Entity CRR ", "acronyms": [[70, 76], [138, 141]], "long-forms": [[53, 68]]}, {"text": "Semantic Feature Performance  The semantic features include Named Entity  (NE), Noun Hypernym (NHype) and Head Verb  Synset (HVSyn).", "acronyms": [[95, 100], [75, 77], [125, 130]], "long-forms": [[80, 93], [60, 72], [106, 123]]}, {"text": " We consider two resources for training the RCM term: the Paraphrase Database (PPDB) (Ganitkevitch et al, 2013) and WordNet (Fell-", "acronyms": [[79, 83]], "long-forms": [[58, 77]]}, {"text": "Figure 1: Na??ve Bayes Model The model described above is commonly known as a na??ve Bayes (NB) model. NB models have", "acronyms": [[92, 94], [103, 105]], "long-forms": [[78, 90], [10, 22]]}, {"text": "DSTG = Adverb s t r i r ig   mRTOVO = For + Subject -+ to -+ Object  NA = N t- Adjective  NASOBJBE - N + as - t- Object  of be -", "acronyms": [[69, 71], [0, 4], [29, 35], [90, 98]], "long-forms": [[74, 88]]}, {"text": "We also used two Korean speech recognizers: a speech recognizer made by LG-Elite (LG Electronics Institute of Technology) and a Korean commercial speech recog-", "acronyms": [[72, 80]], "long-forms": [[82, 106]]}, {"text": "Where is that??).  Finally, the task features (TASK) reflect conflicting instructions in the domain.", "acronyms": [[47, 51]], "long-forms": [[32, 36]]}, {"text": "posed web-based semantic similarity measures: Jaccard, Dice, Overlap, PMI (Bollegala et al, 2007), Normalized Google Distance (NGD) (Cilibrasi and Vitanyi, 2007), Sahami and Heil-", "acronyms": [[127, 130], [70, 73]], "long-forms": [[99, 125]]}, {"text": "as dependants. Dependency structures are suit-  ably depicted as a directed acyclic graph(DAG),  where arrows direct from dependants to gover- ", "acronyms": [[90, 93]], "long-forms": [[67, 89]]}, {"text": "qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, np), qdet?1.0 DET(the),", "acronyms": [[52, 54], [8, 10], [30, 32], [76, 79]], "long-forms": [[56, 60]]}, {"text": "This convexity given the n4 Discounted cumulative gain (DCG) is widely used in information retrieval learning-to-rank settings.", "acronyms": [[56, 59]], "long-forms": [[28, 54]]}, {"text": " {UK, US, AU}.   Figure 1: Proposed architecture      Figure 2: Baseline GMM based dialect classification 5.2 Latent Semantic Analysis for Dialect ID One approach used to address topic classification problems has been latent semantic analysis (LSA), which was first explored for document indexing in (Deerwester et al, 1990). This addresses the issues of synonymy - many ways to refer to the same idea and polysemy ?", "acronyms": [[244, 247], [2, 4], [6, 8], [10, 12], [73, 76], [147, 149]], "long-forms": [[218, 242]]}, {"text": " 1 Introduct ion  Some natural anguage processing (NLP) tasks can  be performed with only coarse-grained semantic in- ", "acronyms": [[51, 54]], "long-forms": [[23, 49]]}, {"text": "coverage of the course material.  The quality estimation task (QET) (CallisonBurch et al 2012) aims to develop quality indica-", "acronyms": [[63, 66]], "long-forms": [[38, 61]]}, {"text": "ratio of the number of completely corrected generated MIUs over the number of all MIUs, and character accuracy (Ch-Acc), but the sentence accuracy (S-Acc) will also be reported in evaluation", "acronyms": [[112, 118], [54, 58], [82, 86], [148, 153]], "long-forms": [[92, 110], [129, 146]]}, {"text": "edge in Y .  The Wall Street Journal Penn Treebank (PTB) (Marcus et al, 1993) contains parsed constituency", "acronyms": [[52, 55]], "long-forms": [[37, 50]]}, {"text": "probabilistic Earley?s, and minimum edit distance algorithms). Dynamic programming (DP) involves solving certain kinds of recursive equations", "acronyms": [[84, 86]], "long-forms": [[63, 82]]}, {"text": "up in the bracketed terminal string as insertion o f \"  (* *) \" surround-  ing \" 1970 \".) Inverse noun phrase preposing (NPPREPOS), which  relates such surface pairs as \" GM's sales \" and \" the sales of GM \", ", "acronyms": [[121, 129], [171, 173], [203, 205]], "long-forms": [[98, 119]]}, {"text": "terms encountered.  Shallow Syntactic (SSyn) features consider the number and ratios of common part-of-speech", "acronyms": [[39, 43]], "long-forms": [[20, 37]]}, {"text": "Table 9 (Hindi). Here, precision measures the number of correct Named Entities (NEs) in the machine tagged file over the total number of NEs in the ma-", "acronyms": [[80, 83], [137, 140]], "long-forms": [[64, 78]]}, {"text": "lem of selecting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that crosslingual word embeddings will allow for accurate", "acronyms": [[128, 130]], "long-forms": [[102, 126]]}, {"text": "BUS (business) Belgium Labour Federation 9 4440 11.0 NOB (nobility) Albert II 6 4179 15.1 COM (comics) Suske and Wiske 3 4000 10.5 MUS (music) Sandra Kim, Urbanus 3 1296 14.6", "acronyms": [[90, 93], [0, 3], [53, 56], [131, 134]], "long-forms": [[95, 101], [5, 13], [58, 66], [136, 141]]}, {"text": "operating system.  For production deployment we  used Message Driven Beans (MDBs)using IBM  Websphere Application Server? (", "acronyms": [[76, 80], [87, 90]], "long-forms": [[54, 74]]}, {"text": "it is the in f in i t ive  form of a verb. If SO, it is to be attached to the  parsing tree, and given the additionql feature MVB (main verb). The current ", "acronyms": [[126, 129], [46, 48]], "long-forms": [[131, 140]]}, {"text": "their semantic deviation values. The result is a  list of pairs called the ICS (Initial Cluster Set). ", "acronyms": [[75, 78]], "long-forms": [[80, 99]]}, {"text": "(Associativity)  \\[A\\[BC\\]\\] = \\[\\[AB\\]C\\]  (A(BC)) = ((AB)C)  (L, -singleton bidirectionality) ", "acronyms": [[56, 60]], "long-forms": [[45, 49]]}, {"text": " In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040?1047, Uppsala, Sweden, July.", "acronyms": [[93, 96]], "long-forms": [[50, 91]]}, {"text": "distribution which underlies natural language text   -- which is if not a pure Zipfian distribution at least  an LNRE (large number of rare events, cf. Baayen ", "acronyms": [[113, 117]], "long-forms": [[119, 146]]}, {"text": "for the three sponsoring agencies. The TIPSTER  Research and Evaluation Committee (REC) was  charged with oversight responsibility of the 15 ", "acronyms": [[83, 86]], "long-forms": [[48, 81]]}, {"text": "to) Peet a friend?  Figure 10: An argument post particle phrase (PP) (upper) and an adjunct PP (lower).", "acronyms": [[65, 67], [92, 94]], "long-forms": [[48, 63]]}, {"text": "other animate and inanimate), which cover ten categories of noun phrases, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other", "acronyms": [[115, 119], [95, 98], [133, 136]], "long-forms": [[121, 127], [100, 112], [150, 158]]}, {"text": " Entities On the level of entity extraction, Named Entities (NE) were defined as proper names and quantities of interest. ", "acronyms": [[61, 63]], "long-forms": [[45, 59]]}, {"text": "see chapter 4.3.  WIV(1): Weighted Identity Value (with the weight 1):  see chapter 2.2.", "acronyms": [[18, 21]], "long-forms": [[26, 49]]}, {"text": "n?5\u0001WZWZ7V?Zo+Y#?ZWA<\u0007E  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216?1224, October 25-29, 2014, Doha, Qatar.", "acronyms": [[113, 118]], "long-forms": [[63, 111]]}, {"text": "the expected output.  formally a weighted finite state automation (FSA), where V is the set of nodes andE is the set of edges.", "acronyms": [[67, 70]], "long-forms": [[42, 65], [120, 124]]}, {"text": "In our experiments, we used the Penn Treebank (PTB) (Marcus et al 1993) for English and the Chinese Treebank version 5.1 (CTB5) (Xue et al 2005) for Chinese.", "acronyms": [[122, 126], [47, 50]], "long-forms": [[92, 120], [32, 45]]}, {"text": "cannot co-exist on nouns. Next comes the class of particle proclitics (PART+): +  l+ ?", "acronyms": [[71, 76]], "long-forms": [[50, 69]]}, {"text": "otherwise as uniform as possible (Berger et al, 1996). maximum entropy model (MaxEnt) is known to easily combine diverse features and", "acronyms": [[78, 84]], "long-forms": [[55, 70]]}, {"text": "TGTM P=p,pk ,b   TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l   TGTM PW=pw, pkw, bw ", "acronyms": [[49, 51], [0, 4], [17, 21], [22, 24], [44, 48], [74, 78], [79, 81]], "long-forms": [[53, 56], [25, 27], [82, 84]]}, {"text": "  2 NII-Speech Resources Consortium  The National Institute of Informatics (NII) was  founded in Tokyo, Japan in April 2000 as an in-", "acronyms": [[76, 79], [4, 7]], "long-forms": [[41, 74]]}, {"text": "6 6   Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 79?83, Seoul, South Korea, 5-6 July 2012.", "acronyms": [[102, 109]], "long-forms": [[52, 100]]}, {"text": "(2) works7. This algorithm is evaluated in (Jin and Tanaka-Ishii, 2006) using Peking University (PKU) 7Three segmentation criteria are given in (Jin and Tanaka-", "acronyms": [[97, 100]], "long-forms": [[78, 95]]}, {"text": "For example, NNS  (noun ? plural) became NN (noun). ", "acronyms": [[41, 43], [13, 16]], "long-forms": [[45, 49], [19, 25]]}, {"text": "is associated with the data sparseness problem. Most of the previously proposed methods to  extract compounds or to measure word association using mutual information (MI) either ignore  or penalize items with low co-occurrence ounts (Church and Hanks 1990, Su, Wu and Chang ", "acronyms": [[167, 169]], "long-forms": [[147, 165]]}, {"text": " 3.1 Identifying verbal blocks (Vbs) Verbal blocks are composed of a head (Vb-H) and possibly accompanying dependents (Vb-D).", "acronyms": [[75, 79], [32, 35], [119, 123]], "long-forms": [[37, 73], [17, 30]]}, {"text": "ously capture the semantics of words and sentences, such as latent semantic analysis (LSA) or latent Dirichlet alocation (LDA). However, our", "acronyms": [[122, 125], [86, 89]], "long-forms": [[94, 120], [60, 84]]}, {"text": "http://www.cs.ualberta.ca/?yx2/research.html 59 class-based hidden Markov model (HMM) model (Zhang et al, 2003).", "acronyms": [[81, 84]], "long-forms": [[60, 79]]}, {"text": " To solve this problem, we introduce  Document oriented Preference Sets(DoPS). The ", "acronyms": [[72, 76]], "long-forms": [[38, 70]]}, {"text": " 4.4 SLU Features The SLU (Spoken Language Understanding) features are used to resolve implicit and explicit REs.", "acronyms": [[22, 25], [5, 8], [109, 112]], "long-forms": [[27, 56]]}, {"text": "To create Bloom filter LMs we gathered n-gram counts from both the Europarl (EP) and the whole of the Gigaword Corpus (GW). Table 2 shows the", "acronyms": [[119, 121]], "long-forms": [[102, 110]]}, {"text": "from Association. ACM Transactions on Information Systems (TOIS) 21:315-346. ", "acronyms": [[59, 63], [18, 21]], "long-forms": [[22, 57]]}, {"text": "2   TASK A: Question Generation from Paragraphs  1.1   Task Definition  The Question Generation from Paragraphs (QGP) task challenges participants to  generate a list of 6 questions from a given input paragraph.", "acronyms": [[113, 116]], "long-forms": [[76, 111]]}, {"text": "TH + DR + EG + LC 56.51 TH + EG + LC 56.50 Character Type (CT) 51.96 Word Familiarity (WF) 51.50", "acronyms": [[59, 61], [0, 2], [5, 7], [10, 12], [15, 17], [24, 26], [29, 31], [34, 36], [87, 89]], "long-forms": [[43, 57], [69, 85]]}, {"text": "semaatic grounds, new referent objects must be created. The number of objects to be  created is set equal to the QTY (quantity) attribute of the noun phrase if specified (as in  \"two boys\" (P20)), to two if the noun phrase is plural and not compound, to the number ", "acronyms": [[113, 116]], "long-forms": [[118, 126]]}, {"text": "ing. In Proceedings of the A CL Fifth Conference  on Applied Natural Language Processing (ANLP),  pages 139-146, Washington, DC.", "acronyms": [[90, 94], [29, 31], [125, 127]], "long-forms": [[53, 88]]}, {"text": "variance distribution. In the feature space, a support vector machine (SVM) classifier (Vapnik, 1995) is used to determine the likelihoods", "acronyms": [[71, 74]], "long-forms": [[47, 69]]}, {"text": "Abstract  We present a novel method for evaluating  the output of Machine Translation (MT),  based on comparing the dependency ", "acronyms": [[87, 89]], "long-forms": [[66, 85]]}, {"text": "do you have? for taskswitching (SWT) and poker-playing (PKR) respectively.", "acronyms": [[32, 35], [56, 59]], "long-forms": [[21, 30], [41, 46]]}, {"text": "........ ? ................... ? ............... + ................... +  CLS = Clause NP = Noun Phrase (BAR 2)  PROD = Predicate $SUBJ -- SUBJECT value of feature ~OL ", "acronyms": [[87, 89], [74, 77], [105, 108], [113, 117], [130, 135], [165, 167]], "long-forms": [[92, 103], [80, 86], [120, 129], [139, 146]]}, {"text": "Commentary We distinguish three types of events in the domain: identification (ID) events trigger the system to name the street the car is on, turn events fire", "acronyms": [[79, 81]], "long-forms": [[63, 77]]}, {"text": "We have followed their methodology as best as we could, using the same WordNet (WN) categories and the same corpora.", "acronyms": [[80, 82]], "long-forms": [[71, 78]]}, {"text": "resented by an order domain (DOM), which is a list  of domain objects, whose relative order must satisfy  a set of linear precedence (LP) constraints. The or- ", "acronyms": [[134, 136], [29, 32]], "long-forms": [[115, 132], [21, 27]]}, {"text": "characterize the AAV functions mediating this effect, cloned AAV type 2 wild-type or mutant genomes were transfected into simian virus 40 (SV40)-transformed hamster cells together with the six HSV replication genes", "acronyms": [[139, 143], [17, 20], [61, 64], [193, 196]], "long-forms": [[122, 137]]}, {"text": " A more flexible approach is to do it in two steps: complementation, forming a VP (verb phrase) from the verb and the object, and predication", "acronyms": [[79, 81]], "long-forms": [[83, 94]]}, {"text": "RLP = rule  learner prediction. RS = Reference Standard   ", "acronyms": [[32, 34], [0, 3]], "long-forms": [[37, 55], [6, 30]]}, {"text": "more, 1976; Fillmore, 1982). The classes of events are the semantic frames of the lexical units (LUs) that evoke them, and the roles associated with the", "acronyms": [[97, 100]], "long-forms": [[82, 95]]}, {"text": "We also wanted to determine if information about 6http://www.isi.edu/?ravichan/YASMET.html dialog acts (DA) helps the ranking task. If we", "acronyms": [[104, 106]], "long-forms": [[91, 102]]}, {"text": "2007; Noh and Pad?o, 2013).  2.2 Entailment Core (EC) The Entailment Core performs the actual entail-", "acronyms": [[50, 52]], "long-forms": [[33, 48]]}, {"text": " 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle", "acronyms": [[28, 30]], "long-forms": [[16, 26]]}, {"text": "pathway representation formats: Systems Biology Markup Language (SBML)3 (Hucka et al, 2003) and Biological Pathway Exchange (BioPAX)4 (Demir et al, 2010).", "acronyms": [[125, 131], [65, 69]], "long-forms": [[96, 123], [32, 63]]}, {"text": "for Answer Search (IYAS) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Hamad Bin Khalifa University and", "acronyms": [[131, 135], [19, 23], [81, 84]], "long-forms": [[95, 129], [51, 79], [0, 17]]}, {"text": "In this paper, we also implement type raising rules for compact representation of PP (prepositional phrase) and AP (adverbial phrase).", "acronyms": [[82, 84], [112, 114]], "long-forms": [[86, 106], [116, 132]]}, {"text": "1 This work has been developed in the project KFr-FAST (KIT = Kilnstliche  Intelligenz und Textverstehen (Artificial Intelligence and Text  Understanding); FAST = Functor Argument S ructure for Translation), which  constitutes the Berlin component of the complementary research project of ", "acronyms": [[156, 160]], "long-forms": [[163, 189]]}, {"text": "erences to the instructors in the posts etc.  3.3 Linear Chain Markov Model (LCMM) The logistic regression model is good at exploit-", "acronyms": [[77, 81]], "long-forms": [[50, 75]]}, {"text": "176 Figure 2: The embedding graph for the sentence Our previous results show that recombinant gp41 (aa565-647), the extracellular domain of HIV-1 transmembrane glycoprotein, stimulates interleukin-10 (IL-10) production in human monocytes.", "acronyms": [[201, 206], [140, 145]], "long-forms": [[185, 199]]}, {"text": "3.1 Feature Structure To implement the twin model, we adopt the log linear or maximum entropy (MaxEnt) model (Berger et al, 1996) for its flexibility of combining diverse sources of", "acronyms": [[95, 101]], "long-forms": [[78, 93]]}, {"text": "Table 2  Summary of error rates with the language model only (LM), the prosody model only (PM), the  combined ecision tree (CM-DT), and the combined HMM (CM-HMM). ( a) shows word-based ", "acronyms": [[124, 129], [154, 160], [62, 64], [91, 93]], "long-forms": [[101, 122], [140, 152], [41, 55], [71, 84]]}, {"text": "hardware) but also not highly specialized (implying that it is not limited too severely in scope of use). We believe  an architecture composed of a distributed set of processing elements (PE's), each containing local memory and high  speed DSP processors, with a limited interconnecfion a d communication capability may suit our needs.", "acronyms": [[188, 192], [240, 243]], "long-forms": [[167, 186]]}, {"text": "the features N, V, or 0 which indicate a nominal parent (NP or PP), a verbal parent (VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is propagated to the head child.", "acronyms": [[155, 158], [57, 59], [63, 65], [85, 87]], "long-forms": [[136, 153], [70, 83], [41, 55]]}, {"text": "tive of the gold standard data.  Finally, the alignment error rate (AER) is lower (and hence better) for English?French than Romanian?", "acronyms": [[68, 71]], "long-forms": [[46, 66]]}, {"text": "corrccmess as follows:  ? Translation Correctness (TA). This is tile percentage of", "acronyms": [[51, 53]], "long-forms": [[26, 37]]}, {"text": " An arguably better approach to representation learning is Canonical Correlation Analysis (CCA) that induces representations that are maximally cor-", "acronyms": [[91, 94]], "long-forms": [[59, 89]]}, {"text": "rithms can also be used in discriminative settings (Bellare et al, 2009; Ganchev et al, 2010) specifically for semi-supervised learning (SSL.) ", "acronyms": [[137, 141]], "long-forms": [[116, 135]]}, {"text": "This work was partly supported by UK EPSRC project GR/N36462/93: ? Robust Accurate Statistical Parsing (RASP)?. ", "acronyms": [[104, 108], [34, 36], [37, 42]], "long-forms": [[67, 102]]}, {"text": " 9 Here, we present the generation-oriented PG Workbench (PGW), which assists grammar developers, among other things, in testing whether the implemented syntactic and lexical knowledge allows all and only well-formed permutations. In Section 2, we describe PG?s topology-based linearizer implemented in the PGW gen-erator, whose software design is sketched in Section 3.", "acronyms": [[58, 61], [307, 310], [257, 261]], "long-forms": [[44, 56]]}, {"text": "al., 2005; Joachims et al, 2009) formulation, as shown in Optimization Problem 1 (OP1), to learn a weight vector w.", "acronyms": [[82, 85]], "long-forms": [[58, 80]]}, {"text": "trieval process. ( Zhou and Wade, 2009b) proposed a Latent Dirichlet Allocation (LDA)based method to model the latent structure of ", "acronyms": [[81, 84]], "long-forms": [[52, 79]]}, {"text": "8 Fully Supervised Model 53.7 70.8 61.1 53.0 70.3 60.4 52.0 69.6 59.6 53.1 70.5 60.6 Table 4: Results obtained using the MUC scoring program for the Broadcast News and Newswire data sets Broadcast News (BNEWS) Newswire (NWIRE) True Mentions System Mentions True Mentions System Mentions", "acronyms": [[203, 208], [220, 225], [121, 124]], "long-forms": [[187, 201], [210, 218]]}, {"text": "the predicted margin. Dredze and Crammer (2008a) showed how Confidence Weighted (CW) learning could be used to generate a more informative mea-", "acronyms": [[81, 83]], "long-forms": [[60, 79]]}, {"text": "TI = terse information  INT = interrupted  TRUN = truncated  TRANS = transposed sentence (discussed in ", "acronyms": [[43, 47], [0, 2], [24, 27], [61, 66]], "long-forms": [[50, 59], [5, 22], [30, 41], [69, 79]]}, {"text": "2 Conditional Random Fields 2.1 The model Conditional Random Fields(CRFs), a statistical sequence modeling framework, was first intro-", "acronyms": [[68, 72]], "long-forms": [[42, 66]]}, {"text": "ing different linguistic aspects and an effective way to combine such information is made available by Support Vector (SV) regression, described in (Smola and Scho?lkopf, 2004).", "acronyms": [[119, 121]], "long-forms": [[103, 117]]}, {"text": " Definition 1  A lexical conceptual structure (LCS) is a modified version of the representation proposed  by Jackendoff (1983, 1990) that conforms to the following structural form: ", "acronyms": [[47, 50]], "long-forms": [[17, 45]]}, {"text": "5.2.1 Query Focused Rewards We have proposed an extension to both reward functions to allow for query focused (QF) summarization.", "acronyms": [[111, 113]], "long-forms": [[96, 109]]}, {"text": "vincent.claveau@irisa.fr christian.raymond@irisa.fr R?SUM?Dans cet article, nous d?crivons notre participation au D?fi Fouille de Texte (DeFT) 2012. Ced?fi consistait en l?attribution automatique de mots-cl?s ?", "acronyms": [[137, 141], [52, 57]], "long-forms": [[81, 135]]}, {"text": "We have tested our algorithms on both the handcoded tag set used in (Chen et al, 1999) and supertags extracted for Penn Treebank(PTB). On the", "acronyms": [[129, 132]], "long-forms": [[115, 128]]}, {"text": "structure come in two different types: ? Grammatical Functions (GFs) indicate the relationship between the predicate and depen-", "acronyms": [[64, 67]], "long-forms": [[41, 62]]}, {"text": " 5.1 Calculation of Emotion Tag weights  Sense_Tag_Weight (STW): The tag weight has  been calculated using SentiWordNet.", "acronyms": [[59, 62]], "long-forms": [[41, 57]]}, {"text": "1093  Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 1?9, Gothenburg, Sweden, April 26-30 2014.", "acronyms": [[91, 96], [25, 29]], "long-forms": [[47, 89]]}, {"text": "Based on RenCECps (an annotated emotion corpus) and MaxEnt (Maximum entropy) model, several contextual features and their com-", "acronyms": [[52, 58], [9, 17]], "long-forms": [[60, 75]]}, {"text": "The second approach is based on statistical modeling. We adopted one typical  implementation called the \"vector space model\" (VSM) (Frakes and Baeza-Yates 1992;  Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch/itze 1992), which has ", "acronyms": [[126, 129]], "long-forms": [[105, 123]]}, {"text": "tions (Abe et al, 1996).  1.1 Question answering (QA) Unlike IR systems which return a list of documents", "acronyms": [[50, 52]], "long-forms": [[30, 48]]}, {"text": "On the one hand, we built machine learning classifiers based on Support Vector Machines (SVMs) and Conditional Random Fields (CRFs).", "acronyms": [[89, 93], [126, 130]], "long-forms": [[64, 87], [99, 124]]}, {"text": "4 Experiments 4.1 Event Extraction We extract events by running the Semantic Role Labeling (SRL) tool in SENNA (Collobert et al., 2011).", "acronyms": [[92, 95], [105, 110]], "long-forms": [[68, 90]]}, {"text": "INAN=inanimate NP, ANIM=animate NP, VBZ--inflected  main verb, IS=is, VBG=gerund, PP=prepositional phrase,  TO=to (prep.), ONmon (prep.).", "acronyms": [[108, 110], [0, 4], [15, 17], [19, 23], [32, 34], [36, 39], [63, 65], [70, 73], [82, 84]], "long-forms": [[111, 113], [5, 14], [24, 31], [66, 68], [74, 80], [85, 105]]}, {"text": "we combine different perspectives, the performance improves and we use the L+S with SVR for run 1 (LSSVR), L+P+S with SVR for run 2 (LPSSVR), and L+P+S with SVR using transductive learning", "acronyms": [[99, 104], [84, 87], [133, 139], [157, 160]], "long-forms": [[107, 129]]}, {"text": "! JIM: { Person67 / Person83 / Name18 / (TYPE=&Person, SEX=Male, NAME=NamelS) }  Finally, the association between grammatical functions and ", "acronyms": [[65, 69]], "long-forms": [[70, 76]]}, {"text": "portance of the edge features and the resultant largemargin constraint, we also compare against a standard binary Support Vector Machine (SVM) which uses node features alone to predict whether each", "acronyms": [[138, 141]], "long-forms": [[114, 136]]}, {"text": "Sane: ..PERJANTAINA (pltkanl iper jantalna)  PER3ANTAI FRIDAY Noun $8 Eaa  Sane: PITK&KSI.. (p i tk ika iper Janta iks i )   PITK& LI3NG Ad ject ive  $8 Trane l  ", "acronyms": [[81, 91], [125, 130]], "long-forms": [[93, 120]]}, {"text": "Table 1: Mean relative frequencies and standard deviation for each class (A(nimate) vs. I(nanimate)) from feature extraction (SUBJ=Transitive Subject, OBJ=Object, GEN=Genitive -s, PASS=Passive byphrase, ANAAN=Anaphoric reference by animate pronoun, ANAIN=Anaphoric reference by inanimate pronoun, REFL=Anaphoric reference by reflexive pronoun).", "acronyms": [[203, 208], [126, 130], [151, 154], [163, 166], [180, 184], [249, 254], [297, 301]], "long-forms": [[209, 239], [155, 161], [167, 175], [185, 192], [255, 287], [325, 334]]}, {"text": "Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus. ", "acronyms": [[146, 148], [15, 17], [36, 39], [80, 85], [120, 128], [181, 183]], "long-forms": [[149, 168], [18, 34], [40, 67], [86, 111], [129, 137], [184, 186]]}, {"text": "BLEU uses 2 reference translations. WER=word error rate, PER=position independent WER. ", "acronyms": [[82, 85]], "long-forms": [[61, 81]]}, {"text": "#and (a probabilistic AND) is used. Otherwise, the  probabilistic passage operator #UWn (unordered window)  is used.", "acronyms": [[84, 87], [22, 25]], "long-forms": [[89, 105]]}, {"text": "71 tweets, called T-NER, is presented which employs Conditional Random Fields (CRF) for named entity segmentation and labelled topic modelling for", "acronyms": [[79, 82], [18, 23]], "long-forms": [[52, 77]]}, {"text": "trol agreement principle.  Consider first the foot feature principle (FFP). ", "acronyms": [[70, 73]], "long-forms": [[46, 68]]}, {"text": "simultaneous (SML) with another proposition: \"Fred washed the car  while John chased Mary\", Figure 50 A sequenttal rn iering of proposi-  t ions is also found, characterized by a sequence (SEQ) relation. The ", "acronyms": [[189, 192], [14, 17]], "long-forms": [[179, 187], [0, 12]]}, {"text": "Abstract Verbal comprehension questions appear very frequently in Intelligence Quotient (IQ) tests, which measure human?s verbal ability includ-", "acronyms": [[89, 91]], "long-forms": [[66, 87]]}, {"text": "1998. Protein folding in the hydrophobic-hydrophilic(HP) model is NPcomplete.", "acronyms": [[53, 55], [66, 68]], "long-forms": [[29, 52]]}, {"text": " 1 Introduction  Open domain question answering (QA), as defined  by the TREC competitions (Voorhees, 2003), ", "acronyms": [[49, 51], [73, 77]], "long-forms": [[29, 47]]}, {"text": "In Proceedings of the 22nd International Conference on World Wide Web (WWW), pp. 1009-1020, 2013.", "acronyms": [[71, 74]], "long-forms": [[55, 69]]}, {"text": "ring Expression Generation Task (TUNA-REG) organised by Albert Gatt, Anja Belz and Eric Kow; the two GREC Challenges, GREC Main Subject Reference Generation (GREC-MSR) and GREC Named Entity Generation (GREC-NEG), organised by Anja Belz, Eric Kow, Jette Viethen and Albert Gatt; and the Giving Instructions in", "acronyms": [[202, 210], [33, 41], [101, 105], [118, 122], [158, 166]], "long-forms": [[172, 200]]}, {"text": "Typically, the weights of the log-linear combination in Equation 3 are optimised by means of Minimum Error Rate Training (MERT) (Och, 2003).", "acronyms": [[122, 126]], "long-forms": [[93, 120]]}, {"text": "Table 6: GAP scores for compared methods.  UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston", "acronyms": [[43, 45], [55, 57], [70, 72], [9, 12], [86, 88], [100, 102]], "long-forms": [[48, 53], [60, 68], [75, 84], [91, 98], [105, 121]]}, {"text": " ? National Drug File7 (NDF): this ontology  contains information about a comprehensive ", "acronyms": [[24, 27]], "long-forms": [[3, 22]]}, {"text": "2005; Wieling et al, 2007) for string similarity  estimation, and is based on the notion of string  Edit Distance (ED). String ED is defined here as ", "acronyms": [[115, 117], [127, 129]], "long-forms": [[100, 113]]}, {"text": "We consider two common ways of calculating the translation probability: using the maximum likelihood estimator (MLE) and smoothing the MLE using lexical weighting.", "acronyms": [[112, 115], [135, 138]], "long-forms": [[82, 110]]}, {"text": "4.1 Preprocessing  HTML Page Parsing  The Document Object Model (DOM) is an application programming interface used for parsing ", "acronyms": [[65, 68], [19, 23]], "long-forms": [[42, 63]]}, {"text": "Argument Filtering Argument  Boundary Detection (ABD) module ?)???????", "acronyms": [[49, 52]], "long-forms": [[29, 47]]}, {"text": "For each bracketed phrase, if its FF label does not  fit into the corresponding default pattern, (like for  the noun phrase(NP), the default grammatical  structure is that the last noun in the phrase is the ", "acronyms": [[124, 126], [34, 36]], "long-forms": [[112, 123]]}, {"text": "found in each row, as well as the level of granularity of analysis in each row.3 2KEY: ABS=abstract, COM=completive, CL=classifier, DEM=demonstrative, E=ergative, EV=evidential, S=singular,", "acronyms": [[87, 90], [101, 104], [117, 119]], "long-forms": [[91, 99], [105, 115], [120, 130]]}, {"text": "2: boston sweep colorado to win world series 3: rookies respond in first crack at the big time C-LR=C-LexRank; WDS=Word Distributional Similarity Table 4: Top 3 ranked summaries of the redsox cluster", "acronyms": [[111, 114], [95, 99]], "long-forms": [[115, 145], [100, 109]]}, {"text": "  (a)  Support vector machine (SVM) (Vapnik,  1998) is a supervised learning algorithm proposed ", "acronyms": [[31, 34]], "long-forms": [[7, 29]]}, {"text": "by the connectives will yield better readability.  Entity Grid (EG) Along with the previous work (Pitler and", "acronyms": [[64, 66]], "long-forms": [[51, 62]]}, {"text": "2 Long Short-Term Memory Networks 2.1 Overview Recurrent neural networks (RNNs) are able to process input sequences of arbitrary length via the re-", "acronyms": [[74, 78]], "long-forms": [[47, 72]]}, {"text": "2005a; Jeon et al, 2005b) compared four different  retrieval methods, i.e. vector space model, Okapi,  language model (LM), and translation-based model,  for automatically fixing the lexical chasm between ", "acronyms": [[119, 121]], "long-forms": [[103, 117]]}, {"text": " 5 Conclusions Multiword expressions (MWEs) are a major obstacle that hinder precise natural language processing", "acronyms": [[38, 42]], "long-forms": [[15, 36]]}, {"text": "sentences. The third, following (Yates et al, 2006), is maximum recall (MR). MR simply predicts that all", "acronyms": [[72, 74], [77, 79]], "long-forms": [[56, 70]]}, {"text": "2.2 Natural Language Processing  2.2.1 Woods' Augmented Transition Networks  Thc Augmented Transition Network (ATN) of Woods [Woods 501 is a  powerful formalisnl for representing grammars.", "acronyms": [[111, 114]], "long-forms": [[81, 109]]}, {"text": "particularly helpful in parsing where the sequence  of words forming the MWE is treated as a single  word with a single part of speech (POS) tag. MWE ", "acronyms": [[136, 139], [73, 76], [146, 149]], "long-forms": [[120, 134]]}, {"text": "for each mention, four pieces of information: 1. the mention type: person (PER), organization (ORG), location (LOC), geopolitical en-", "acronyms": [[75, 78], [95, 98], [111, 114]], "long-forms": [[67, 73], [81, 93], [101, 109]]}, {"text": "Abstract  This paper describes a reestimation method for stochastic language models uch as the  N-gram model and the Hidden Maxkov Model(HMM) from ambiguous observations. It is ", "acronyms": [[137, 140]], "long-forms": [[117, 135]]}, {"text": "C, N, X, Y } (V = verb, AV = auxiliary verb, EV = verb with Ersatzinfinitiv, Vfin = finite verb, Vinf", "acronyms": [[24, 26], [45, 47], [77, 81]], "long-forms": [[29, 43], [50, 75], [84, 95], [18, 22]]}, {"text": "Diacritization evaluation of our experiments is  reported in terms of word error rate (WER), and  diacritization error rate (DER)5. ", "acronyms": [[125, 128], [87, 90]], "long-forms": [[98, 123], [70, 85]]}, {"text": "line debates. In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM)@ EACL, pages 35?43.", "acronyms": [[88, 92], [95, 99]], "long-forms": [[52, 86]]}, {"text": "Lima or Jessica Alba??. Therefore, we decided to employ a Conditional Random Fields (CRF) tagger (Lafferty et al, 2001) to the task, since CRF", "acronyms": [[85, 88], [139, 142]], "long-forms": [[58, 83]]}, {"text": "matical relations on the arcs).  Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corre-", "acronyms": [[55, 57]], "long-forms": [[33, 53]]}, {"text": " In Proceedings of the Conference on Web Search and Web Data Mining (WSDM). ", "acronyms": [[69, 73]], "long-forms": [[37, 67]]}, {"text": "Y? >l LY?l, s.t. SYl = SY?l (1) 1", "acronyms": [[17, 20]], "long-forms": [[23, 27]]}, {"text": "ambiguation as a binary classification problem. Li  then uses Support Vector Machine (SVM) with  mutual information between each Chinese charac-", "acronyms": [[86, 89]], "long-forms": [[62, 84]]}, {"text": "EXPERIMENTS 3.1. Speaker Identification (SID) In order to investigate robust speaker identification under", "acronyms": [[41, 44]], "long-forms": [[17, 39]]}, {"text": "above.  Iconic Inflectional Classes (IICs) are ICs that are manually fully annotated, i.e., they have all the tem-", "acronyms": [[37, 41]], "long-forms": [[8, 35]]}, {"text": "NP NP\\NP (S\\NP )/NP NP< >NP S\\NP <S Fruit flies like bananas NP S\\NP (S\\S)/NP NP< >S S\\S <S (b) The search space in this work, with one node for each", "acronyms": [[64, 68], [0, 2], [3, 8], [61, 63], [25, 27], [28, 32]], "long-forms": [[70, 77]]}, {"text": " To overcome this problem, Gliozzo et al (2005) introduced the domain model (DM) and show how to define a domain VSM in which texts and terms", "acronyms": [[77, 79], [113, 116]], "long-forms": [[63, 75]]}, {"text": "systems. This shared task was partially supported by Shared Annotated Resources (ShARe) project NIH 5R01GM090187 and Temporal His-", "acronyms": [[81, 86], [96, 99]], "long-forms": [[53, 79]]}, {"text": "pus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC). ", "acronyms": [[128, 131], [5, 8]], "long-forms": [[108, 126]]}, {"text": "Abbreviations: CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, BLem=BioLemmatizer, Snowball=Snowball stemmer, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion annotation process introduced by Pyysalo et al", "acronyms": [[177, 179], [15, 22], [41, 47], [64, 68], [84, 92], [111, 116], [151, 159]], "long-forms": [[180, 199], [32, 39], [48, 54], [69, 82], [93, 101], [117, 142], [160, 168]]}, {"text": "Table 4: Accuracies on a balanced test set (random baseline: 0.5). NB = NaiveBayes. JRip?2 = Jripper with ?", "acronyms": [[67, 69], [84, 88]], "long-forms": [[72, 82], [93, 100]]}, {"text": "and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used suc-", "acronyms": [[80, 84]], "long-forms": [[58, 78]]}, {"text": "treebank. In: Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24? ", "acronyms": [[86, 89]], "long-forms": [[51, 84]]}, {"text": "ary, we assign a default value 3.0.  3.2 Named Entities (NE)  Named Entities are important semantic information ", "acronyms": [[57, 59]], "long-forms": [[41, 55]]}, {"text": "5 23   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69, Sofia, Bulgaria, August 9, 2013.", "acronyms": [[72, 79]], "long-forms": [[38, 70]]}, {"text": "These features have been obtained using the knowledge contained into the Multilingual Central Repository (MCR) of the MEANING project3 (Atserias et al, 2004).", "acronyms": [[106, 109]], "long-forms": [[73, 104]]}, {"text": "1992. 100 million words of  English: the British National Corpus (BNC). ", "acronyms": [[66, 69]], "long-forms": [[41, 64]]}, {"text": " In (Speriosu et al., 2011), a label propagation (LProp) approach is proposed, while Go et al. ( 2009)", "acronyms": [[50, 55]], "long-forms": [[31, 48]]}, {"text": " 4.1 KNN classification The basic idea of the K nearest neighbor (KNN) classification algorithm is to use already categorized", "acronyms": [[66, 69], [5, 8]], "long-forms": [[46, 64]]}, {"text": "[byline : SAN SALVADOR, 19 APR 89 (ACAN-EFE) --] [bracket : [TEXT]] [fullname : SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIIII] CONDEMNED THE TERRORIST KILLING OF [fullname : ATTORIEY GENERAL ROBERTO GARCI A ALVARADO] AND (comp : ACCUSED THE FARABUIDO MARTI NATIONAL LIBERATION FROIT [bracket : (FMLN)) OF) THE CRIME . ", "acronyms": [[294, 298]], "long-forms": [[240, 275]]}, {"text": "2 Background  2.1 Gene Expression Programming  Gene Expression Programming (GEP), first introduced by (Ferreira 2001), is an evolutionary algo-", "acronyms": [[76, 79]], "long-forms": [[47, 74]]}, {"text": "~968)). Le SN d~fini  es tmarqu~ par la presence du d~ter-  minati f  d~fini  (DEF), lequel peut ~tre anaphorique (ANAPH),  ou d~monstrat i f  (DEM), selon qu'i l  se r~f~re ~ l 'envi- ", "acronyms": [[115, 120], [11, 13], [79, 82], [144, 147]], "long-forms": [[102, 113], [49, 76], [127, 141]]}, {"text": " 2.1 Multilingual Central Repository The Multilingual Central Repository (MCR)2 follows the model proposed by the EuroWordNet", "acronyms": [[74, 77]], "long-forms": [[41, 72]]}, {"text": "Table 1: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for decipherment of letter", "acronyms": [[118, 123], [29, 32], [56, 59], [75, 77], [153, 157]], "long-forms": [[111, 116], [9, 26], [35, 53], [66, 72]]}, {"text": " The tagging has been done using a GUI-based tool  called the Discourse Tagging Tool (DTTool) ac-  cording to \"The Discourse Tagging Guidelines\" we ", "acronyms": [[86, 92], [35, 38]], "long-forms": [[62, 84]]}, {"text": "syntactically correct) to 1.0 (completely wrong).  ISER (information item semantic error rate): The test sentences are segmented into information items; for each of these items, the translation candidates", "acronyms": [[51, 55]], "long-forms": [[57, 93]]}, {"text": "Learning attitudes and attributes from multi-aspect reviews. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020?1025.", "acronyms": [[77, 81], [89, 93]], "long-forms": [[61, 75]]}, {"text": "Table 1 provides an overview of all entity classes and relations. The workflow consists of two steps: Firstly, rule- and ontology-based named entity recognition (NER) is performed (cf. Section", "acronyms": [[162, 165]], "long-forms": [[136, 160]]}, {"text": "In (Raymond and Riccardi, 2007), the SFST-based model is compared with Support Vector Machines (SVM) (Vapnik, 1998) and Conditional Random Fields (CRF) (Laf-", "acronyms": [[96, 99], [37, 41], [147, 150]], "long-forms": [[71, 94], [120, 145]]}, {"text": "examples in (5). This change in beliefs about the past is treated as an error identification signal (EIS). ", "acronyms": [[101, 104]], "long-forms": [[72, 99]]}, {"text": "Answer Pinpointing. In Proceedings of the DARPA Human Language Technology Conference (HLT). ", "acronyms": [[86, 89], [42, 47]], "long-forms": [[48, 73]]}, {"text": "This paper describes the UNITOR system participating in both tasks of the *SEM 2013 shared task on Semantic Textual Similarity (STS), described in (Agirre et al 2013):", "acronyms": [[128, 131], [25, 31], [75, 78]], "long-forms": [[99, 126]]}, {"text": "feeling (FE)  food (FO)  group (GR)  location (LO) ", "acronyms": [[32, 34], [9, 11], [20, 22], [47, 49]], "long-forms": [[25, 30], [0, 7], [14, 18], [37, 45]]}, {"text": "The projection  of the root node on the active leaves is referred to  as the M-BDU (Main BDU). Only syntactic infor-", "acronyms": [[77, 82]], "long-forms": [[84, 92]]}, {"text": " MEI is one of the four projects elected for  the Johns Hopkins University (JHU) Summer  Workshop 2000.1 Our research focus is on the ", "acronyms": [[76, 79], [1, 4]], "long-forms": [[50, 74]]}, {"text": "Internet: chris@lsi .com NLP OBJECTIVES LSI's overall natural language processing (NLP) objective is the development of a broad coverage, reusable system which is readily transportable to additional domains, applications, and sublanguages in English, as well as", "acronyms": [[83, 86], [25, 28], [40, 43]], "long-forms": [[54, 81]]}, {"text": "determine the appropriate xpressional form. Hovy's text structurer (Hovy 1988b),  for example, uses rhetorical relations as defined in Rhetorical Structure Theory (RST)  (Mann and Thompson 1987) to order a set of propositions to be expressed.", "acronyms": [[164, 167]], "long-forms": [[135, 162]]}, {"text": "the exploration and verbalization history; and (4) it then sends semantic representations in the form of preverbal messages (PVMs) to the Formulation & Articulation components.", "acronyms": [[125, 129]], "long-forms": [[105, 123]]}, {"text": "September/NNP \\] \\[O ./. \\]  we can extract following chunk patterns:  NP=NULL 90 PRP 99 VBZ  VP=PRP 99 VBZ 99 DT ", "acronyms": [[74, 78], [10, 13], [71, 73], [82, 85], [89, 92], [94, 96], [97, 100], [104, 107], [111, 113]], "long-forms": []}, {"text": " 2.3 Linear Programming A Linear Program (LP) is an optimization problem where a linear objective function is minimized (or maximized) under linear constraints.", "acronyms": [[42, 44]], "long-forms": [[26, 40]]}, {"text": "represents the editing cycle. Given a  Semantic Network (SN) in a knowledge base (KB),  the system generates a description of the SN in the ", "acronyms": [[57, 59], [82, 84], [130, 132]], "long-forms": [[39, 55], [66, 80]]}, {"text": " 1 Introduction Spoken Dialogue Systems (SDSs) play a key role in achieving natural human-machine interaction.", "acronyms": [[41, 45]], "long-forms": [[16, 39]]}, {"text": "Instead of using words directly, it is possible to employ a (much smaller) controlled vocabulary: Medical Subject Headings (MeSH), consisting of 22,500 codes, are (mostly) manually assigned to", "acronyms": [[124, 128]], "long-forms": [[98, 122]]}, {"text": "y?i / _ + [+ANY] Features: VWL = vowel ANY = any char.", "acronyms": [[27, 30], [39, 42]], "long-forms": [[33, 38], [45, 53]]}, {"text": "leaves=29). The first branching, which corresponds to no AP (Absolute Position) and no C (Colour), assigns n to as many as 1571 instances", "acronyms": [[57, 59]], "long-forms": [[61, 78], [90, 96]]}, {"text": "perimented with three classifiers available in R?  logistic regression (LogR), decision tree (DTree) and support vector machines (SVM).", "acronyms": [[72, 76], [94, 99], [130, 133]], "long-forms": [[51, 70], [79, 92], [105, 127]]}, {"text": "alignment problems. In Annual Meeting of the Association for Computational Linguistics (ACL), Columbus, Ohio, June.", "acronyms": [[88, 91]], "long-forms": [[45, 86]]}, {"text": "\\canonical\" dependency direction under wellde\fned conditions, distinguishing between ordre lin\u0013eaire (linear precedence(LP)) and ordre structural (immediate dominance(ID)).", "acronyms": [[120, 122]], "long-forms": [[102, 118]]}, {"text": "bic Diacritics. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and the 44th Annual Meeting of the the Association", "acronyms": [[98, 104]], "long-forms": [[71, 96]]}, {"text": "man evaluators 5.2 Mean Absolute Difference Analysis Here we calculated the mean absolute difference(MAD) between a human rater?s evaluation and SELSA (LSA)", "acronyms": [[101, 104], [145, 150], [152, 155]], "long-forms": [[76, 100]]}, {"text": "list of Frames alphabetically surrounding Compliance runs as follows: Compatibility, Competition, Complaining, Completeness, Compliance, Concessive.... Next, we attempt to catalogue the Lexical Units (LUs) associated with the frame. ", "acronyms": [[201, 204]], "long-forms": [[186, 199]]}, {"text": "1 Introduction Newswire text has long been a primary target for natural language processing (NLP) techniques such as information extraction, summarization, and ques-", "acronyms": [[93, 96]], "long-forms": [[64, 91]]}, {"text": "using the DSO corpus, which contains sentences drawn from two different corpora, namely Brown Corpus (BC) and Wall Street Journal (WSJ). They", "acronyms": [[131, 134], [10, 13], [102, 104]], "long-forms": [[110, 129], [88, 100]]}, {"text": "EVG smoothed-skip-head 65.0 (5.7) L-EVG smoothed 68.8 (4.5) Table 1: Directed accuracy (DA) for WSJ10, section 23. ", "acronyms": [[88, 90], [0, 3], [34, 39]], "long-forms": [[69, 86]]}, {"text": "   First Paragraph (FPAR): We examined several  hundred pages, and observed that a human could ", "acronyms": [[20, 24]], "long-forms": [[3, 18]]}, {"text": "4.2 Proposed Model : PNB (vs. UM) Figure 1 shows the performances of our new model named Poisson naive Bayes(PNB) classifiers acTable 2: Performances of UM and PNB on the", "acronyms": [[109, 112], [21, 24], [153, 155], [160, 163], [30, 32]], "long-forms": [[89, 107]]}, {"text": "Ravi and Knight (2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while Corlett and Penn (2010) solve the problem using", "acronyms": [[134, 137]], "long-forms": [[110, 132]]}, {"text": "subset by eliminating the redundant features.  In this paper, Rough Set Theory (RST) based  feature selection method is applied for sen-", "acronyms": [[80, 83]], "long-forms": [[62, 78]]}, {"text": "document is zero.  Agglomerative Hierarchical Clustering (AHC)  AHC is a bottom-up hierarchical clustering ", "acronyms": [[58, 61], [64, 67]], "long-forms": [[19, 56]]}, {"text": " Examples of lexical and contextual rules learned by  the Brill tagger.  NNP = proper noun, CD = cardinal number,  CC = coordinating conjunction, JJ = adjective, VBG = verb, ", "acronyms": [[92, 94], [73, 76], [115, 117], [146, 148], [162, 165]], "long-forms": [[97, 105], [79, 90], [120, 144], [151, 160], [168, 172]]}, {"text": "characters ? are often referred to by pronouns or definite noun phrases (NPs) instead of explicit repetition. ", "acronyms": [[73, 76]], "long-forms": [[59, 71]]}, {"text": "LPM output using application knowledge  ? Function Generator Module (FGM) converting  SAM output into executable function calls ", "acronyms": [[69, 72], [86, 89], [0, 3]], "long-forms": [[42, 67]]}, {"text": " The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the French Treebank (FTB) (Abeill?, Cl?ment, and Kinyon", "acronyms": [[118, 121], [67, 70]], "long-forms": [[101, 116], [45, 65]]}, {"text": "     1 Introduction  Most of the natural language generation (NLG)  components in current dialog systems are imple-", "acronyms": [[62, 65]], "long-forms": [[33, 60]]}, {"text": "2012. Disclosure and Quality of Answers in Text and Voice Interviews on iPhones. 67th Annual Conference of the Ameri-can Association for Public Opinion Research (AAPOR) Orlando, FL. Jongho Shin, Shrikanth Narayanan, Laurie Gerber, Abe Kazemzadeh, and Dani Byrd.", "acronyms": [[162, 167], [178, 180]], "long-forms": [[121, 160]]}, {"text": "In this paper we propose a system which uses  hybrid methods that combine both rule-based  and machine learning (ML)-based approaches  to solve GENIA Event Extraction of BioNLP ", "acronyms": [[113, 115], [144, 149], [170, 176]], "long-forms": [[95, 111]]}, {"text": "ticPhrase, Predicate, Argument, ? the MILE Data Categories (MDC) which constitute the attributes and values to adorn", "acronyms": [[60, 63]], "long-forms": [[38, 58]]}, {"text": "ferently by (1) and (2)8.  5 The Neutral Edge Direction (NED) Measure", "acronyms": [[57, 60]], "long-forms": [[33, 55]]}, {"text": " The specific case focused on in this paper is that of AL with SVMs (AL-SVM) for imbalanced ?", "acronyms": [[69, 75]], "long-forms": [[55, 67]]}, {"text": "learning this decision is learned automatically.  Reinforcement Learning (RL) has been successfully used for learning dialogue management", "acronyms": [[74, 76]], "long-forms": [[50, 72]]}, {"text": "in a fully automatic fashion. Again, this is an exciting possibility that would solve the current bottleneck of supervised word sense disambiguation (WSD) methods (namely, that sense-tagged corpora are very costly to acquire).", "acronyms": [[150, 153]], "long-forms": [[123, 148]]}, {"text": "based on a probabilistic model. We investigate two methods using Latent Dirichlet Allocation (LDA) (Blei, 2003) in ?", "acronyms": [[94, 97]], "long-forms": [[65, 92]]}, {"text": "news headlines (headlines); mapping of lexical resources from Ontonotes to Wordnet (OnWN) and from FrameNet to WordNet (FNWN); and evaluation of machine translation (SMT).", "acronyms": [[120, 124]], "long-forms": [[99, 118]]}, {"text": "that are of interest o specific users. An example of IE is the  Named Ent i ty  (NE) task, which has become established  as the important first step in many other IE tasks, provid- ", "acronyms": [[81, 83], [53, 55], [163, 165]], "long-forms": [[64, 78]]}, {"text": "Turian et al. ( 2010) applied word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (", "acronyms": [[88, 91]], "long-forms": [[62, 86]]}, {"text": " 6 BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire;", "acronyms": [[3, 5], [33, 35], [54, 57], [93, 95]], "long-forms": [[8, 31], [38, 52], [60, 91], [98, 106]]}, {"text": "In our development of 60 Japanese predicates (verb and verbal noun) frequently appearing in Kyoto University Text Corpus (KTC) (Kurohashi and Nagao, 1997) , 37.6% of the frames included", "acronyms": [[122, 125]], "long-forms": [[92, 120]]}, {"text": " 2.2 Tree substitution grammars Tree substitution grammars (TSGs) allow for complementary analysis.", "acronyms": [[60, 64]], "long-forms": [[32, 58]]}, {"text": " After detecting a new indefinite description (as ETA(x) : unlversity(x)) ReP  creates a new \"referential object'\" (RefO). During the discours6 (after the ", "acronyms": [[116, 120], [50, 56], [74, 77]], "long-forms": [[94, 112]]}, {"text": "typically expressed inTopic Statements.  The Subject Field Coder (SFCoder) uses an  establishet~ semantic oding scheme from the machine- ", "acronyms": [[66, 73]], "long-forms": [[45, 64]]}, {"text": "(UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance). ", "acronyms": [[90, 92], [1, 3], [46, 48]], "long-forms": [[95, 111], [6, 15], [51, 60]]}, {"text": "Abstract A number of issues arise when trying to scaleup natural language understanding (NLU) tools designed for relatively simple domains (e.g.,", "acronyms": [[89, 92]], "long-forms": [[57, 87]]}, {"text": "Comprehension in 100 days,? published by  Chung Hwa Book Co., (H.K.) Ltd.  The  ChungHwa training set includes 100 English ", "acronyms": [[63, 67]], "long-forms": [[48, 56]]}, {"text": "4.4.2 Optimization To maximize the objective in (6), we employ a stochastic gradient descent (SGD) algorithm (Rendle et al, 2009).", "acronyms": [[94, 97]], "long-forms": [[65, 92]]}, {"text": "described in Section 3. We then trained linear SVMs (Support Vector Machine) using the LIBLINEAR software (Fan et al, 2008), using L1 loss", "acronyms": [[47, 51], [87, 96]], "long-forms": [[53, 75]]}, {"text": "to this tree as the Hidden Factors Tree (HFT). We use Minimum Discriminative Information (MDI) algorithm (Zitouni, 2007) to build the tree.", "acronyms": [[90, 93], [41, 44]], "long-forms": [[54, 88], [20, 39]]}, {"text": "jansche.1@osu.edu 1 Introduction Our approach to multilingual named entity (NE) recognition in the context of the CoNLL Shared", "acronyms": [[76, 78]], "long-forms": [[62, 74], [114, 119]]}, {"text": "Abstract  There has been a long-standing methodology for  evaluating work in speech recognition (SR), but until  recently no community-wide methodology existed for either ", "acronyms": [[97, 99]], "long-forms": [[77, 95]]}, {"text": "nominal elements. For German, we see confusions with the object functions (accusative OA and dative objects DA), predicates (PD), and the EP function marking expletive pronouns in subject position.", "acronyms": [[125, 127]], "long-forms": [[113, 123]]}, {"text": "Donald, 2006). As a joint approach, we consider Reluctant Trimmer (RT), a simplification system that employs synchronous dependency grammars", "acronyms": [[67, 69]], "long-forms": [[48, 65]]}, {"text": "methods to identify such targets. The first method depends on identifying noun groups (NG). We con-", "acronyms": [[87, 89]], "long-forms": [[74, 85]]}, {"text": "used throughout the paper. The notation is that  used in the Core Language Engine (CLE) devel-  oped by SKI's Cambridge Computer Science Re- ", "acronyms": [[83, 86], [104, 107]], "long-forms": [[61, 81]]}, {"text": "However, this hypothesis is reasonable if the  monolingual wordnets are reliable and correctly  linked to the interlingual index (ILI). Quality ", "acronyms": [[130, 133]], "long-forms": [[110, 128]]}, {"text": "Abstract In this work we present results from using Amazon?s Mechanical Turk (MTurk) to annotate translation lexicons between English", "acronyms": [[78, 83]], "long-forms": [[61, 76]]}, {"text": " In this paper we deal with automatic acquisition of verbal selectional preferences (SPs) for Latin, i. e. the semantic preferences of verbs on their ar-", "acronyms": [[85, 88]], "long-forms": [[60, 83]]}, {"text": "shown by Kusner and colleagues (2015), semantic representations such as Latent Semantic Indexing and Latent Dirichlet Allocation (LDA) can outperform a BoW representation.", "acronyms": [[130, 133], [152, 155]], "long-forms": [[101, 128]]}, {"text": "S# for the count in non-speculative ones)  3 Methods  Conditional random fields (CRF) model was  firstly introduced by Lafferty et al (2001).", "acronyms": [[81, 84]], "long-forms": [[54, 79]]}, {"text": "email: mal@aber.ac.uk Stephen Pulman University of Oxford (UK) email: sgp@clg.ox.ac.uk", "acronyms": [[59, 61]], "long-forms": [[37, 57]]}, {"text": "tistical machine translation. In Proceedings of the Machine Translation Summit (MT-Summit). ", "acronyms": [[80, 89]], "long-forms": [[52, 78]]}, {"text": "84  Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 144?151, Ann Arbor, June 2005.", "acronyms": [[82, 87]], "long-forms": [[41, 80]]}, {"text": "category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department,  GI = Operative Gastrointestinal, RAD = Radiology and ", "acronyms": [[75, 77], [27, 29], [52, 56], [103, 105], [136, 139]], "long-forms": [[80, 100], [32, 49], [59, 73], [118, 134], [142, 151]]}, {"text": "section 8.1). Then, the grammar underlying the parser is provided with a specific attachment heuristic that uses corequirement (CR) information from the lexicon. ", "acronyms": [[128, 130]], "long-forms": [[113, 126]]}, {"text": "Note that for the purposes of Figure 1: Example Babytalk Input Data: Sensors HR = Heart Rate; TcPO2 = blood O2 level; TCPCO2 = blood CO2 level; SaO2 = oxygen saturation; T1 = chest", "acronyms": [[77, 79], [94, 99], [118, 124], [144, 148], [170, 172]], "long-forms": [[82, 92], [102, 110], [127, 136], [151, 168]]}, {"text": "ing and understanding convolutional networks. In European Conference on Computer Vision (ECCV). ", "acronyms": [[89, 93]], "long-forms": [[49, 87]]}, {"text": "We use four groups of datasets. The first group comes from the English Web Treebank (EWT),4 also used in the Parsing the Web shared task", "acronyms": [[85, 88]], "long-forms": [[63, 83]]}, {"text": "As a result, a text to text similarity measure is built based on word to word similarity. It is as-sumed that combining word to word similarity metrics to text level would be a good indicator of text to text similarity. 4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments.", "acronyms": [[352, 355], [232, 234], [256, 258], [373, 376]], "long-forms": [[318, 346]]}, {"text": "posterior distribution.  We utilize maximum entropy (MaxEnt) model  (Berger et al, 1996) to design the basic classifier ", "acronyms": [[53, 59]], "long-forms": [[36, 51]]}, {"text": "exist solely to guide processing. These words, known  as function words (FW's), are quite common, and  include articles, prepositions, and auxiliary verbs.", "acronyms": [[73, 77]], "long-forms": [[57, 71]]}, {"text": "4 Variational vs. Min-Risk Decoding In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR): y?", "acronyms": [[125, 128], [18, 26], [52, 55]], "long-forms": [[105, 123]]}, {"text": "Table 4: Entailment judgment in closed test  of mutual information (T=True, F=False,  MI=mutual information). ", "acronyms": [[86, 88]], "long-forms": [[89, 107], [70, 74], [78, 83]]}, {"text": "1434  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 82?86, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[75, 80], [84, 88]], "long-forms": [[41, 73]]}, {"text": "representation. Thus, it is necessary to define a proper ECS for each  language: Japanese ECS (J-ECS) \\[6\\] for Japanese language and  English ECS (E-ECS) \\[7\\] for English language.", "acronyms": [[95, 100], [57, 60], [148, 153]], "long-forms": [[81, 93], [135, 146]]}, {"text": "that have to be defined in a theory-specific way.  Thus a document representation (DocRep) has two components, a DocAttr and a DocRepSeq.", "acronyms": [[83, 89], [113, 119], [127, 136]], "long-forms": [[58, 81]]}, {"text": "notation is illustrated in Figure 3.  3.3 Positional Unknown Model (PosUnk) The main weakness of the PosAll model is that", "acronyms": [[68, 74]], "long-forms": [[42, 60]]}, {"text": "A block ?[] invokes both the inner and outer generations simultaneously in Bracket Model A (BM-A). ", "acronyms": [[92, 96]], "long-forms": [[75, 90]]}, {"text": "Therefore, identification methods like Tsuchiya et al (2006) which uses Support Vector Machines(SVM) have been proposed to solve this problem.", "acronyms": [[96, 99]], "long-forms": [[72, 94]]}, {"text": "con. In Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci). ", "acronyms": [[84, 90]], "long-forms": [[57, 82]]}, {"text": "sisted of 2000 features, representing the most frequent (head, subject) and (head, object) pairs in the British National Corpus (BNC). The feature-", "acronyms": [[129, 132]], "long-forms": [[104, 127]]}, {"text": "We also include a simple baseline that selects the first document sentence as a caption and show the average caption length (AvgLen) for each model.", "acronyms": [[125, 131]], "long-forms": [[101, 123]]}, {"text": "and the other for the target phrase. We call the unit of the two RAEs the Leaf Node Network (LNN). ", "acronyms": [[93, 96], [65, 69]], "long-forms": [[74, 91]]}, {"text": "Lowe HJ, Barnett GO. ( 1994) Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches.", "acronyms": [[83, 87], [5, 7]], "long-forms": [[57, 81]]}, {"text": "out of this phrase. The word with the parent out of  the phrase is called Head of Phrase (HP). The ", "acronyms": [[90, 92]], "long-forms": [[74, 88]]}, {"text": " These are typically expressed in simple syntactic frames called subcategorization frames (SCFs). ", "acronyms": [[91, 95]], "long-forms": [[65, 89]]}, {"text": "Taking into account the above strategies, we propose three concrete DS to CS conversions: Flat conversion with H auxiliary symbol (FlatH). ", "acronyms": [[131, 136], [68, 70], [74, 76]], "long-forms": [[90, 112]]}, {"text": "event coreference resolver has to rely on the noisy outputs produced by its upstream components in the standard information extraction (IE) pipeline. ", "acronyms": [[136, 138]], "long-forms": [[112, 134]]}, {"text": "For each dataset, we report Pearson?s correlation r with human judgments on pairs that are found in both resources (BOTH). Otherwise, the re-", "acronyms": [[116, 120]], "long-forms": [[100, 104]]}, {"text": " Type Short time members Long time members Abbreviations Husband My DD (Dear Daughter), Your PS (Plastic Surgeon) Social networks Facebook, fb", "acronyms": [[68, 70], [93, 95]], "long-forms": [[72, 85], [97, 112]]}, {"text": "1969)).  4 Ia the following \\] will use the term Unification Grammar (UG) aa hyperonym for  GPSG, LFG, FUG, IIPSG etc.,", "acronyms": [[70, 72], [92, 96], [98, 101], [103, 106], [108, 113]], "long-forms": [[49, 68]]}, {"text": "question answering (QA). However, few models in ad hoc information retrieval (IR) use paths for document ranking due to", "acronyms": [[78, 80], [20, 22]], "long-forms": [[55, 76], [0, 18]]}, {"text": "819 location (LO) of the incident (e.g. airport name), and the country (CO) where the incident occurred. ", "acronyms": [[72, 74], [14, 16]], "long-forms": [[63, 70], [4, 12]]}, {"text": "Lapata, 2006). All systems were controlled to produce similar compression ratios (CR) for fair comparison.", "acronyms": [[82, 84]], "long-forms": [[62, 80]]}, {"text": "Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al (2010), on this widely-used benchmark corpus are", "acronyms": [[108, 112], [55, 58]], "long-forms": [[96, 106]]}, {"text": " Quasi-valency complementations: \u0000 DIFF (difference): The number has swollen by 200. ", "acronyms": [[35, 39]], "long-forms": [[41, 51]]}, {"text": "(http://ieee.rkbexplorer.com/) repository7. The corpus of cancer research (COCR) contains 3334 domain specific abstracts of scientific publica-", "acronyms": [[75, 79]], "long-forms": [[48, 73]]}, {"text": "Reduced Sentences 0.121 0.055 4.89 0.027* 1.129 1.01 to 1.26 Constant 5.23 1.18 19.67 <0.000* 187.25 ADAG, n=242; HAG, n = 242; S.E = standard error; OR = Odds ratio or Exp(?); CI = confidence Interval.", "acronyms": [[128, 131], [150, 152], [114, 117], [177, 179], [101, 105]], "long-forms": [[134, 148], [155, 165], [182, 201]]}, {"text": "Manning (2014) baseline. These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (?", "acronyms": [[71, 77], [90, 94], [117, 120]], "long-forms": [[45, 69]]}, {"text": "4.1 Compositional Neural Language Model (C-NLM) Compositional Neural Language Model (C-NLM) is a combination of a word representation learning", "acronyms": [[85, 90], [41, 46]], "long-forms": [[48, 83], [4, 39]]}, {"text": " Each verb-noun pair was presented to turkers via Amazon Mechanical Turk (AMT) and they were asked to describe (by text) the changes of", "acronyms": [[74, 77]], "long-forms": [[50, 72]]}, {"text": " We  also  produced  an  upper  bound  using  Naive  Bayes  multinomial  (NBm)  and Support Vector Machine (SVM)6 classifiers  with the NTU Sentiment  Dictionary (Ku  et al, ", "acronyms": [[108, 111], [74, 77], [136, 139]], "long-forms": [[84, 106], [46, 71]]}, {"text": "as defined in Section 2. For tagging, we experimented with Support Vector Machines (SVM) and Conditional Random Fields (CRF).", "acronyms": [[84, 87], [120, 123]], "long-forms": [[59, 82], [93, 118]]}, {"text": "organizations in knowledge mining approaches to  master this information for quality assurance or  Customer Relationship Management (CRM) purposes.", "acronyms": [[133, 136]], "long-forms": [[99, 131]]}, {"text": " 2.5 Noise Contrastive Estimation Noise contrastive estimation (NCE) is another sampling-based technique (Hyv?arinen, 2010;", "acronyms": [[64, 67]], "long-forms": [[34, 62]]}, {"text": "  SVM Classification  SVM (Support Vector Machines) has attracted  much attention since it was introduced in (Boser et ", "acronyms": [[22, 25], [2, 5]], "long-forms": [[27, 50]]}, {"text": " ITSPOKE-WOZ is a semi-automatic version of ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), which is a speech-enhanced ver-", "acronyms": [[44, 51], [1, 12]], "long-forms": [[53, 80]]}, {"text": "lion tokens to 0.3832 for 237 million tokens. At such data sizes, Stupid Backoff (SB) with a constant backoff parameter ?", "acronyms": [[82, 84]], "long-forms": [[66, 80]]}, {"text": "gradient method. In The Conference on Advances in Neural Information Processing Systems (NIPS). ", "acronyms": [[89, 93]], "long-forms": [[50, 87]]}, {"text": "4  At the highest level, the text is a request addressed to CCC  members to vote against making the nuclear freeze initiative (NFI)  one of the issues about which CCC actively lobbies and promotes ", "acronyms": [[127, 130], [60, 63], [163, 166]], "long-forms": [[100, 125]]}, {"text": " TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding", "acronyms": [[58, 60], [36, 39], [1, 5]], "long-forms": [[45, 56], [15, 34]]}, {"text": "general idea widely used in this kind of systems: if  an input sentence is syntactically ill-formed, i.e. it  cannot be assigned a syntactic structure (SyntS),  the system considers minimal changes that enable it ", "acronyms": [[152, 157]], "long-forms": [[131, 150]]}, {"text": "\u0007\u0002 B \u0007\u001eE B \u0004\u001e\u0014\t\u0005E B \u0002\u0014 B \u0019\u0002F\u0013\u0003E\b\u0007 B AE EA B \u0017\u0015 B \u0005\u0013\u0003\u0003\u0006\b) B ,\u0002\u0014 B \tA\u0007E\u0014\b\t\u0007\u0006 EA\u0015 B \u0004E\u0014\u001a\u0002\u0014\u0003\u0006\b) B F\u0002\u0003\u0004\u0002\bE\b\u0007+$\u0006\u0005EB \u0003\u0013A\u0007\u0006\u0004A\u0006F\t\u0007\u0006\u0002\bH0 B \u0002\u001a B$\u0002\u0014\u0019 B  EF\u0007\u0002\u0014\u0005 B ,\u001bF\u001eI\u0007JE\" B /66B0& B(\u001e\u0006\u0005 B \u001e\t\u0005 B \u0007\u001eE B \t\u0019 \t\b\u0007\t)E B \u0007\u001e\t\u0007 B \u0006\u0007 B \u0006\u0005 B \u0006\u0003\u0003E\u0019\u0006\t\u0007EA\u0015B \t\u0004\u0004A\u0006F\t\u0017AEB\u0007\u0002B\t\b\u0015B\u0003\u0002\u0019EAB\u0006\bB$\u001e\u0006F\u001eB$\u0002\u0014\u0019B\u0003E\t\b\u0006\b)BF\t\bB\u0017EBED\u0004\u0014E\u0005\u0005E\u0019B\t\u0005B\tB EF\u0007\u0002\u0014&B(\u001eEB\u0004\u0014\u0006\bF\u0006\u0004AEBA\u0006\u0003\u0006\u0007\t\u0007\u0006\u0002\bB \u0002\u001aBF\u0002\u0013\u0014\u0005EB\u0006\u0005B\u0007\u001e\t\u0007B\u0007\u001e\u0006\u0005B\u0007\t'E\u0005B\b\u0002B\tFF\u0002\u0013\b\u0007B\u0002\u001aB$\u0002\u0014\u0019B\u0002\u0014\u0019E\u0014&B(\u001eE\u0014E\u001a\u0002\u0014E\"B$\u001e\u0006AEB\u0006\u0007B\u001e\t\u0005B\u0004\u0014\u0002 E\bBE\u001a\u001aEF\u0007\u0006 EB\u0006\bB\u0007\u001eEB", "acronyms": [[300, 307]], "long-forms": [[309, 326]]}, {"text": "in all kind of NLP applications. As a particular case, Semantic Role Labeling (SRL) is currently a welldefined task with a substantial body of work and", "acronyms": [[79, 82], [15, 18]], "long-forms": [[55, 77]]}, {"text": "data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split", "acronyms": [[118, 123], [35, 37], [71, 74]], "long-forms": [[101, 116], [18, 33]]}, {"text": "evidences, making the coupled approach efficient enough to be applied to themore complex task of joint word segmentation (WS) and POS tagging for the first time.", "acronyms": [[122, 124], [130, 133]], "long-forms": [[103, 120]]}, {"text": "paring to BL system (N.S.), the average number of alternative translations of each source phrase (T/S) and the average source phrase length in the output (A.L.) -1.80 on average TER.", "acronyms": [[155, 159], [10, 12], [21, 25], [98, 101], [178, 181]], "long-forms": [[111, 139], [62, 89]]}, {"text": "  In the late 1970s a research team at USCInformation Sciences Institute (ISI) studied natural  dialogues with particular interest in applying the ", "acronyms": [[74, 77], [39, 42]], "long-forms": [[42, 72]]}, {"text": "Furthermore, to create a fully text-bound subset, family memberships relations (MEMBER) were resolved into single edges and suitable references to", "acronyms": [[80, 86]], "long-forms": [[57, 78]]}, {"text": "aKemp|es  de  t. raL~uct lon  du  russe  eD f ranca ls  rea l  i s le  par   le  G~TA a Grenob le ,   MUTS-CLES:  TAO ( t raduct ion  ass i s t ,  co  par  o rd lnateur )e   ana lyse  morpno l  og ique~ t rans fer t  lex lca l~ 0enerat  1o~ ", "acronyms": [[114, 117], [81, 85], [102, 111]], "long-forms": [[120, 168]]}, {"text": "changes and there is a complement particle between complement constituents(COP)  and verbs. So in the information dictionary, the characteristics of {V(CHA), VP+COP}  should be described.", "acronyms": [[152, 155], [75, 78], [161, 164], [158, 160]], "long-forms": [[130, 145], [51, 73]]}, {"text": "POS tag distribution. We also use features based on part of speech (POS). We tag using", "acronyms": [[68, 71], [0, 3]], "long-forms": [[52, 66]]}, {"text": "true positive (TP) (i.e., a correct match), and an appropriate NNS triple not found in the gold standard set a false negative (FN) (i.e., an incorrect nonmatch), as shown in Table 4.", "acronyms": [[127, 129], [15, 17], [63, 66]], "long-forms": [[111, 125], [0, 13]]}, {"text": " Finally, we propose using the beam-search decoder to combine multiple discriminative models such as M3N and multiple generative models such as language models (LM) and perform multiple passes of disfluency detection.", "acronyms": [[161, 163], [101, 104]], "long-forms": [[144, 159]]}, {"text": " For each combination, we measure the attachment score (AS) and the exact match (EM). A signif-", "acronyms": [[81, 83], [56, 58]], "long-forms": [[68, 79], [38, 54]]}, {"text": "Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1?47.", "acronyms": [[74, 78], [51, 54]], "long-forms": [[55, 72]]}, {"text": "occurs from different production sources, we  propose an extension to this genre of technique  in the form of a Group Sparse Model (GSM)  which enforces sparsity with a L2,1 norm instead ", "acronyms": [[132, 135]], "long-forms": [[112, 130]]}, {"text": "seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) procedure.", "acronyms": [[131, 133]], "long-forms": [[105, 129]]}, {"text": "Translation. In Proceedings of the 13th International  Conference on Computational Linguistics (COLING-90),  Vol.", "acronyms": [[96, 105]], "long-forms": [[55, 94]]}, {"text": "In Proceedings of Workshop on Setting Standards for Searching Electronically Stored Information In Discovery Proceedings (DESI-4). ", "acronyms": [[122, 128]], "long-forms": [[99, 120]]}, {"text": "email: elenimi@linc.cis.upenn.edu Jerry R. Hobbs University of Southern California (USA) email: hobbs@isi.edu", "acronyms": [[84, 87]], "long-forms": [[49, 82]]}, {"text": "Maximum Entropy Markov Model (MEMM)-based word segmenter with Conditional Random Fields (CRF)based chunking; 3.", "acronyms": [[89, 92]], "long-forms": [[62, 87]]}, {"text": "that is neither terminal nor lexical. An interior node is  said to meet he foot condition (FC) iffeach foot feature  that it contains appears also on at least one daughter ", "acronyms": [[91, 93]], "long-forms": [[75, 89]]}, {"text": "dependency analyzer, respectively.  (V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG =  ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP, ", "acronyms": [[62, 66], [85, 90], [144, 147], [224, 228], [248, 252]], "long-forms": [[69, 83], [93, 103], [56, 60], [41, 50], [151, 159], [164, 172], [231, 246], [255, 267]]}, {"text": "   (5) Flattened CPT (FCPT): the CPT with the  single in and out arcs of non-terminal nodes (ex-", "acronyms": [[22, 26], [33, 36]], "long-forms": [[7, 20]]}, {"text": "Entropy Guided Transformation Learning (ETL) is a new machine learning strategy that combines the advantages of Decision Trees (DT) and Transformation-Based Learning (TBL) (dos Santos", "acronyms": [[128, 130], [40, 43], [167, 170]], "long-forms": [[112, 126], [0, 38], [136, 165]]}, {"text": "refers to the fact that either a particular lexical item or a particular grammatical construction must be present for the omission of a frame element (FE) to occur.", "acronyms": [[151, 153]], "long-forms": [[136, 149]]}, {"text": "single weight matrixW . In contrast, the CVG uses a syntactically untied RNN (SU-RNN) which has a set of such weights.", "acronyms": [[78, 84]], "long-forms": [[52, 76]]}, {"text": "entire web corpus. We use the API provided by the Measures of Semantic Relatedness (MSR)4 engine for this purpose.", "acronyms": [[84, 87], [30, 33]], "long-forms": [[50, 82]]}, {"text": "Acknowledgment This work is supported by the 6th Framework Research Program of the European Union (EU), LUNA Project, IST contract no 33549,www.ist-luna.eu", "acronyms": [[99, 101], [104, 108], [118, 121]], "long-forms": [[83, 97]]}, {"text": "tracted from a corpus of parsed sentences.  A super abstract role value (SuperARV) is an abstraction of the joint assignment of dependencies for", "acronyms": [[73, 81]], "long-forms": [[46, 71]]}, {"text": "Stochastic Gradient Descent (SGD) (Tsuruoka et al., 2009) `2 regularization, hinge loss Bernoulli Naive Bayes (B-NB) (McCallum et al., 1998) ?", "acronyms": [[111, 115], [29, 32]], "long-forms": [[88, 109], [0, 27]]}, {"text": "structural and behavioral parts have to be fully specified at this level.  2.3 Eclipse Modeling Framework (EMF) We chose to adopt EMF as the underlying modeling framework and tooling for our model-driven ap-", "acronyms": [[107, 110], [130, 133]], "long-forms": [[79, 105]]}, {"text": "/NN ?? /NN ]   Input: wi: word index (ID) in a given sentence. ", "acronyms": [[38, 40]], "long-forms": [[31, 36]]}, {"text": "mentation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English,", "acronyms": [[139, 142], [163, 166], [172, 175]], "long-forms": [[122, 130], [145, 154]]}, {"text": " 1 Introduction Named entity recognition (NER) is the task of identifying and classifying phrases that denote certain types of named", "acronyms": [[42, 45]], "long-forms": [[16, 40]]}, {"text": "J08b 97.74 93.37 N07 97.83 93.32 SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score. ", "acronyms": [[33, 35], [60, 62], [88, 91]], "long-forms": [[38, 52], [65, 70]]}, {"text": "Hyderabad, India Abstract Named Entity Recognition (NER) is the task of identifying and classifying all proper", "acronyms": [[52, 55]], "long-forms": [[26, 50]]}, {"text": "Technology (NAST), Lao PDR  ? Madan Puraskar Pustakalaya (MPP),  Nepal  ", "acronyms": [[58, 61]], "long-forms": [[30, 56]]}, {"text": " 2 Abstract Syntax Trees We describe abstract syntax trees (ASTs) using an example from CFR Section 610.11:", "acronyms": [[60, 64], [88, 91]], "long-forms": [[37, 58]]}, {"text": " IV. RETROSPECTIVE SSL (R-SSL). After", "acronyms": [[24, 29]], "long-forms": [[5, 22]]}, {"text": "erol (DAG)?). Note that the position of the long form (LF) and short form (SF) is interchangeable. To dis-", "acronyms": [[75, 77], [6, 9], [55, 57]], "long-forms": [[63, 73], [44, 53]]}, {"text": "summarization. During these intervening decades,  progress in Natural Language Processing (NLP), coupled  with great increases of computer memory and speed, ", "acronyms": [[91, 94]], "long-forms": [[62, 89]]}, {"text": "We trained the UKP machine learning classifier originally developed for the Semantic Textual Similarity (STS) task at SemEval-2012 (B?r et al 2012) on the averaged binary and senary judge-", "acronyms": [[105, 108], [15, 18], [118, 130]], "long-forms": [[76, 103]]}, {"text": "the bird comes.  Table 2: Stories generated by a system that uses plots and genetic search (PlotGA), a system that uses only plots (Plot), McIntyre and Lapata (2009)?s rank-based system (Rank) and a system that randomly", "acronyms": [[92, 98]], "long-forms": [[66, 90]]}, {"text": "data as described in Niehues and Vogel (2008).  The phrase table (PT) is built using the Moses toolkit (Koehn et al.,", "acronyms": [[66, 68]], "long-forms": [[52, 64]]}, {"text": "In Proc. of Seventh Text REtrieval Conference (TREC-7). ", "acronyms": [[47, 53]], "long-forms": [[12, 45]]}, {"text": "build a bridge between UNL and one of the  internal representations of ETAP, namely  Normalized Syntactic Structure (NormSS), and in  this way link UNL with all other levels of text ", "acronyms": [[117, 123], [23, 26], [71, 75], [148, 151]], "long-forms": [[85, 115]]}, {"text": "non relevant texts has the lower expectation. Figure 1 describes the probability density function (PDF ) for domain frequency scores of the SPORT domain", "acronyms": [[99, 102], [140, 145]], "long-forms": [[69, 97]]}, {"text": "other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis", "acronyms": [[124, 127], [68, 71]], "long-forms": [[96, 122], [42, 66]]}, {"text": "be compared, for any section of the corpus. The tool also calculates the majority tag (MajTag). Av-", "acronyms": [[87, 93]], "long-forms": [[73, 85]]}, {"text": "2003) ? Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set, target", "acronyms": [[37, 41]], "long-forms": [[8, 35]]}, {"text": "Relative standard deviation of three intervals, left edge to anchor (LE-A), center to  anchor (CC-A), right edge to anchor (RE-A) calculated across productions of word sets by one ", "acronyms": [[124, 128], [69, 73], [95, 99]], "long-forms": [[102, 122], [48, 67], [76, 93]]}, {"text": "Due to the existence of CTB-I, we were able to train new automatic Chinese language processing (CLP) tools, which crucially use annotated corpora as training", "acronyms": [[96, 99], [24, 29]], "long-forms": [[67, 94]]}, {"text": "for evaluating the ASR,  2. Concept F-measure (ConF) ? the F-measure of ", "acronyms": [[47, 51], [19, 22]], "long-forms": [[28, 37]]}, {"text": "hdaume,marcu \u0001 @isi.edu Abstract Entity detection and tracking (EDT) is the task of identifying textual mentions", "acronyms": [[64, 67]], "long-forms": [[33, 62]]}, {"text": "School of Computer Science, University of Manchester, UK ? National Centre for Text Mining (NaCTeM), UK ?", "acronyms": [[92, 98], [54, 56], [101, 103]], "long-forms": [[59, 90]]}, {"text": " To overcome the difficulty, we build a new Multilayer Search Mechanism (MSM). Different", "acronyms": [[73, 76]], "long-forms": [[44, 71]]}, {"text": "ambiguous verb structure in a garden-path in two ways; one is as a subordinate clause (MV), the other is a Reduced Relative (RR). He defined", "acronyms": [[125, 127], [87, 89]], "long-forms": [[107, 123]]}, {"text": "syntactic skeleton defined in Eq. 1, namely, Subject(S), Verb(V), Direct Object(DO), Indirect Object(IO), Preposition(P) and Noun(Object) of the Preposition(N).", "acronyms": [[80, 82], [101, 103]], "long-forms": [[66, 78], [85, 99], [45, 52], [57, 61], [106, 117]]}, {"text": "on Chinese FrameNet is divided into the subtasks of boundary identification(BI) and semantic role classification(SRC). ", "acronyms": [[113, 116], [76, 79]], "long-forms": [[84, 112], [52, 75]]}, {"text": " 6 Related Work  Boosting is a machine learning (ML) method that  has been well studied in the ML community ", "acronyms": [[49, 51], [95, 97]], "long-forms": [[31, 47]]}, {"text": "229  Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 19?26, Sofia, Bulgaria, August 9, 2013.", "acronyms": [[70, 77]], "long-forms": [[36, 68]]}, {"text": "example, the prepObject )f a LOCATION-PP must be a  PLACE-NOUN. A description of \"on AI\" (as in \"book on  AI\") as a LOCATION-PP c~Id  not be constructed since AI ", "acronyms": [[85, 88], [159, 161], [29, 40], [52, 62], [116, 127]], "long-forms": []}, {"text": "con using label propagation and ground truth EPA values (POS= part-of-speech, W= the number of the induced words, MAS=mean absolute error, and RMSE= root mean squared error", "acronyms": [[114, 117], [45, 48], [57, 60], [143, 147]], "long-forms": [[118, 131], [62, 76], [107, 112], [149, 172]]}, {"text": "to find words with the same meanings. We use a simple approach called the Direct Reversal (DR) approach in (Lam and Kalita, 2013) to create", "acronyms": [[91, 93]], "long-forms": [[74, 89]]}, {"text": "puts to a system and the outputs it is intended to produce. In Machine Translation (MT), such resources take the form of sentence-aligned parallel corpora of", "acronyms": [[84, 86]], "long-forms": [[63, 82]]}, {"text": "ACL 2006 paper (see experiments). Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities", "acronyms": [[62, 65], [0, 3]], "long-forms": [[34, 60]]}, {"text": "In Proc. of the Association for Computational Linguistics (ACL), pages 523?530.", "acronyms": [[59, 62]], "long-forms": [[16, 57]]}, {"text": "We have been concentrating on the Hong  Kong Hansard, which are the parliamentary pro-  ceedings of the Legislative Council (LegCo). Anal- ", "acronyms": [[125, 130]], "long-forms": [[104, 123]]}, {"text": "BP = log (Prob(N2 | N1)) (2) ? Pointwise Mutual Information (PMI): Defined as a measure of how collocated two words are,", "acronyms": [[61, 64], [0, 2]], "long-forms": [[31, 59], [5, 28]]}, {"text": "query large amounts of data is a key requirement for data-driven dialog systems, in which the data is generated by the spoken dialog system (SDS) components (spoken language understanding (SLU), di-", "acronyms": [[141, 144], [189, 192]], "long-forms": [[119, 139], [158, 187]]}, {"text": "to five possible values (rules have been presented with the sentence pairs from which they have been acquired): entailment=yes (YES), i.e. correctness of the rule; entailment=more-phenomena (+PHEN), i.e.", "acronyms": [[128, 131], [192, 196]], "long-forms": [[123, 126], [180, 189]]}, {"text": "system components goes through GDM, thereby insu-  lating parts from each other and providing a uniform  API (applications programmer interface) for manip-  ulating the data produced by the system.", "acronyms": [[105, 108], [31, 34]], "long-forms": [[110, 143]]}, {"text": "The work described in this paper is based on the output of Inputlog3, but it can also be applied to the output of other keystroke-logging programs.  To promote more linguistically-oriented writing process research, Inputlog aggregates the logged process data from the character level (keystroke) to the word level.  In a subsequent step, we use various Natural Language Processing (NLP) tools to further annotate the logged process data with different kinds of linguistic information: part-of-speech tags, lemmata, chunk boundaries, syllable boundaries, and word frequency.  The remainder of this paper is structured as follows.", "acronyms": [[382, 385]], "long-forms": [[353, 380]]}, {"text": "slightly higher F-measures than the UMass dictionary.  The error rates (ERR) for all three dictionaries were  identical.", "acronyms": [[72, 75]], "long-forms": [[59, 70]]}, {"text": "sHDP 0.162 0.046 0.442 0.102 Table 2: Average topic coherence for various baselines (HDP, Gaussian LDA (G-LDA)) and sHDP. ", "acronyms": [[104, 109], [116, 120], [85, 88], [0, 4]], "long-forms": [[90, 102]]}, {"text": "contains the result for Eisner?s algorithm using no transformation (N-Proj), projectivized training data (Proj), and pseudo-projective parsing (P-Proj). The", "acronyms": [[144, 150], [68, 74], [106, 110]], "long-forms": [[124, 142]]}, {"text": "NLP track report. In Proceedings of the 5th  Text REtrieval Conference (TREC-5). ", "acronyms": [[72, 78], [0, 3]], "long-forms": [[40, 70]]}, {"text": "networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). ", "acronyms": [[92, 96], [32, 36]], "long-forms": [[51, 90]]}, {"text": "gorithm used belongs to the family of algorithms described by Covington (2001), and the classifiers are trained using support vector machines (SVM) (Vapnik, 1995).", "acronyms": [[143, 146]], "long-forms": [[118, 141]]}, {"text": "several systems have been developed such as BITS (Bilingual Internet Test Search) (Ma and Liberman, 1999), PTMiner (Parallel Text Miner) (Chen and Nie, 2000), and STRAND (Structural Translation", "acronyms": [[107, 114], [44, 48], [163, 169]], "long-forms": [[116, 135], [50, 80], [171, 193]]}, {"text": " In Section 3, we report on two Amazon Mechanical Turk (MTurk) experiments, which demonstrate that crowdsourcing is a feasible way", "acronyms": [[56, 61]], "long-forms": [[39, 54]]}, {"text": "We further assume that these documents have been tagged with Named Entities (NE)s with coreferences resolved 807", "acronyms": [[77, 79]], "long-forms": [[61, 75]]}, {"text": " This  suspens ion takes place dur ing un i f icat ion  of  the Flat Concurrent  Pro log (FCP) pred icate   (see below), into which expert  rout ines are ", "acronyms": [[90, 93]], "long-forms": [[64, 88]]}, {"text": "press first-order logic. This requirement motivates our use of Inductive Logic Programming (ILP), a learning algorithm capable of inferring logic pro-", "acronyms": [[92, 95]], "long-forms": [[63, 90]]}, {"text": "Since we are going to be  concerned with definability, we first translate CFGs  into CFTs (Context Free Theories). The ", "acronyms": [[85, 89], [74, 78]], "long-forms": [[91, 112]]}, {"text": "The next four columns show the number of true positives (TP)--verbs judged +S both  by machine and by hand; false positives (FP)--verbs judged +S by machine, -S  by  hand; true negatives (TN)--verbs judged -S  both by machine and by hand; and false  negatives (FN)--verbs judged -S  by machine, +S by hand.", "acronyms": [[188, 190], [57, 59], [125, 127], [261, 263]], "long-forms": [[172, 186], [41, 54], [108, 122], [243, 258]]}, {"text": "The number of sentences with product features  ? Word level (WL)  ?", "acronyms": [[61, 63]], "long-forms": [[49, 59]]}, {"text": "The straight case is the one mentioned above, treating all elements on the PARTS list equally (EQUAL). As a second op-", "acronyms": [[95, 100]], "long-forms": [[86, 93]]}, {"text": "In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC). ", "acronyms": [[89, 93]], "long-forms": [[54, 72]]}, {"text": "The/AT table\\]NN is/BEZ ready/J/./.  (PPS = subject pronoun; MD = modal; V'B =  verb (no inflection); AT = article; NN = noun;  BEZ ffi present 3rd sg form of \"to be\"; Jl = ", "acronyms": [[102, 104], [116, 118], [38, 41], [61, 63], [73, 76]], "long-forms": [[107, 114], [121, 125], [66, 71], [80, 84], [44, 59]]}, {"text": "employing them simultaneously. We also include the oracle word error rate (WER) of the WCNs and lattices for each ASR configuration.", "acronyms": [[75, 78], [87, 91], [114, 117]], "long-forms": [[58, 73]]}, {"text": "compiled two datasets consisting of research papers from two top-tier machine learning conferences: World Wide Web (WWW) and Knowledge Discovery and Data Mining (KDD).", "acronyms": [[116, 119], [162, 165]], "long-forms": [[100, 114], [125, 153]]}, {"text": "Proper noun (PropN): yes when the description has a capitalized initial.  Restrictive postmodification (RPostm): yes if the definite description is modified by relative or associative clauses.", "acronyms": [[104, 110], [13, 18]], "long-forms": [[74, 102], [0, 11]]}, {"text": "colour histograms derived from images.  In the RGB (Red Green Blue) colour model, each pixel is represented as an integer in range of", "acronyms": [[47, 50]], "long-forms": [[52, 66]]}, {"text": "Linguistic expressions can vanish and appear in translation. For example, the preposition (PP) in the source rule does not show up in any of   Machine Translation Based on Constraint-Based Synchronous Grammar 615 ", "acronyms": [[91, 93]], "long-forms": [[78, 89]]}, {"text": "to-fine n-Best Parsing and MaxEnt Discriminative Reranking Proceedings of the 43th Annual Meeting on Association for Computational Linguistics(ACL) 173?180.", "acronyms": [[143, 146]], "long-forms": [[101, 141]]}, {"text": "rate words (PERFECT). Second, we let the parser introduce the EEs itself (INSERT). ", "acronyms": [[74, 80], [12, 19]], "long-forms": [[48, 72]]}, {"text": "0 ROOT Figure 2: Example of a CoNLL-style annotated sentence. Each word (FORM) is numbered (ID), lemmatized (LEMMA), annotated with two levels of part-of-speech tags (CPOSTAG and POSTAG), annotated with morpho-", "acronyms": [[92, 94], [30, 35], [73, 77], [109, 114], [167, 174], [179, 185]], "long-forms": [[79, 90], [97, 107], [146, 165]]}, {"text": "4.2 Evaluation of Different Representation Learning Methods Experiment Setup and Dataset We conduct sentiment classification of items in two traditional sentiment lexicons, HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005), to evaluate the effective of the", "acronyms": [[173, 175]], "long-forms": [[177, 187]]}, {"text": "alignment G (both A and G can be split into two subsets AS ,AP and GS , GP , respectively representing Sure and Probable alignments) Precision (PT ), Recall (RT ), F-measure (FT ) and Alignment Error", "acronyms": [[144, 146], [56, 58], [60, 62], [67, 69], [72, 74], [158, 160], [175, 177]], "long-forms": [[112, 131], [150, 156], [164, 173]]}, {"text": " As the noun or adjective occur in the first slot  of conjunct verbs (ConjVs) construction, the  search starts from the point of noun or adjec-", "acronyms": [[70, 76]], "long-forms": [[54, 68]]}, {"text": " These algorithms are now getting keen atten-  tion from the natural anguage processing (NLP)  research community since the huge text corpus ", "acronyms": [[89, 92]], "long-forms": [[61, 87]]}, {"text": "Table 5: Sample Hindi complex predicates   5 Corpus and pre-processing  Basic Travel Expressions Corpus (BTEC)  containing travel conversations is used for ", "acronyms": [[105, 109]], "long-forms": [[72, 103]]}, {"text": "FA8750-09-C-0181. The first author also thanks the Vietnam Education Foundation (VEF) for its sponsorship.", "acronyms": [[81, 84]], "long-forms": [[51, 79]]}, {"text": "Table 1. Coverage and accuracy of each derived feature for RTE3 revised development collection  (RTE3Devmt), the RTE3 Test collection (RTE3Test ) and the entire RTE2 collection (RTE2All). ", "acronyms": [[135, 143], [59, 63], [97, 106], [161, 165], [178, 185]], "long-forms": [[113, 133]]}, {"text": "S i 3.3.3 Method 3: TrueSkill (TS) TrueSkill is an adaptive, online system that em-", "acronyms": [[31, 33]], "long-forms": [[20, 29]]}, {"text": "We focused on syntax, esp. noun phrase (NP) syntax from the beginning.", "acronyms": [[40, 42]], "long-forms": [[27, 38]]}, {"text": " ? REL = relation; ARG = NP/VP/ADJ (6) ACADE?MIQUE = Qui manque d?originalite?,", "acronyms": [[3, 6], [19, 22], [39, 50]], "long-forms": [[9, 17], [25, 34], [53, 78]]}, {"text": "Table 3: Experimental Results (Microsoft?s Provided Train and Test Set) sorted the sentences pairs of the MSRP corpus according to the length difference ratio (LDR) defined in Section 3, and partitioned the sorted cor-", "acronyms": [[160, 163], [106, 110]], "long-forms": [[135, 158]]}, {"text": "reasoning? In International Conference on Learning Representations (ICLR). ", "acronyms": [[68, 72]], "long-forms": [[14, 66]]}, {"text": "Proc. ACM Multimedia (MM), ACM, Florence, Italy. pp.", "acronyms": [[22, 24]], "long-forms": [[15, 20]]}, {"text": "and intangible factors known to the engineer but unknown to the computer. For example, the engineer  may need to postpone the placement of equipment in a certain CSA (Carrier Serving Area) due to a fixed  cap on near-term expenditures, or she may decide to activate DLC (Digital Loop Carrier) equipment in ", "acronyms": [[162, 165]], "long-forms": [[167, 187]]}, {"text": "To perform this relocation quickly, Yata et al. ( 2009) introduced two additional one-dimensional arrays, called NLINK (node link) and BLOCK. For each node, NLINK stores the label needed to reach its", "acronyms": [[113, 118], [157, 162]], "long-forms": [[120, 129]]}, {"text": "idealistic) practice of balancing and purging quirks.  6.2 Lexicography and Exploratory Data Analysis (EDA)  Statistics can be used for many different purposes.", "acronyms": [[103, 106]], "long-forms": [[76, 101]]}, {"text": "? Frequency-based: LUHN (Luhn, 1958) score(S) = maxci?{clusters(S)}{csi}, where", "acronyms": [[19, 23]], "long-forms": [[25, 29], [37, 42]]}, {"text": "In Proceedings of the 2012 ACM Interntional Conference on Intelligent User Interfaces (IUI), pages 189?198. ", "acronyms": [[87, 90], [27, 30]], "long-forms": [[58, 85]]}, {"text": "tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, Massachusetts, USA.", "acronyms": [[89, 93]], "long-forms": [[48, 87]]}, {"text": "strategies for reducing ambiguity.  4.2.1 Longest ending filtering (LEF) The first approach to ambiguity reduction is based", "acronyms": [[68, 71]], "long-forms": [[42, 66]]}, {"text": "In the  past a few year, we have put forward methods for Kazakh morphological analysis, which includes  stem extraction, part of speech(POS) tagging, spelling check, etc. Recently, we are working on syntax ", "acronyms": [[136, 139]], "long-forms": [[121, 134]]}, {"text": "paradigm (Berners-Lee, 2006), which requires the use of uniform resource identifiers (URIs), the hypertext transfer protocol (HTTP), standard representation formats (such as RDF) and links to", "acronyms": [[126, 130], [86, 90], [174, 177]], "long-forms": [[97, 124], [56, 84]]}, {"text": "Many researchers have attempted several tech-  niques to deal with extragrammatical sentences  such as Augmented Transition Network(ATN)  (Kwasny and Sondheimer, 1981), network-based ", "acronyms": [[132, 135]], "long-forms": [[103, 130]]}, {"text": "  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87?95, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[71, 76], [80, 84]], "long-forms": [[37, 69]]}, {"text": "tual distributions (Miller and Charles, 1991). In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,", "acronyms": [[92, 96]], "long-forms": [[60, 90]]}, {"text": "phrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E). For all baselines we", "acronyms": [[116, 119], [80, 83]], "long-forms": [[94, 114], [58, 78]]}, {"text": "of-the-art in more detail.  The field of Information Extraction (IE) has been heavily influenced by the Information Retrieval (IR)", "acronyms": [[65, 67]], "long-forms": [[41, 63]]}, {"text": "(MBF).  4 Multilingual PRF (MultiPRF) The schematic of the MultiPRF approach is shown", "acronyms": [[28, 36], [1, 4], [59, 67]], "long-forms": [[10, 26]]}, {"text": "The score of an abstract based on extracted PICO elements, SPICO, is broken into individual components according to the following formula: SPICO = Sproblem + Spopulation + Sintervention + Soutcome (4) The first component in the equation, Sproblem, reflects a match between the primary", "acronyms": [[139, 144], [44, 48], [59, 64]], "long-forms": [[147, 196]]}, {"text": "The problem of evaluating machine translation output without reference translations is called quality estimation (QE) and has recently been the centre of attention (Bojar et al.,", "acronyms": [[114, 116]], "long-forms": [[94, 112]]}, {"text": "five COMPARE3, and five RECOMMEND CPs.  Each of the 112 textplans (TPs) that produced 4", "acronyms": [[67, 70], [34, 37]], "long-forms": [[56, 65]]}, {"text": "Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of longest span entering nodes.", "acronyms": [[123, 127]], "long-forms": [[98, 121]]}, {"text": "341  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1070?1080, October 25-29, 2014, Doha, Qatar.", "acronyms": [[93, 98]], "long-forms": [[43, 91]]}, {"text": "case.  We choose Support Vector Machines (SVMs) as our learning algorithm for their widely acclaimed", "acronyms": [[42, 46]], "long-forms": [[17, 40]]}, {"text": "1 Introduction Many algorithms in speech and language processing can be viewed as instances of dynamic programming (DP) (Bellman, 1957). The basic idea of", "acronyms": [[116, 118]], "long-forms": [[95, 114]]}, {"text": "uew stngc in the development toward a grammar and  style chc~ker can emerge: the organization of an  algorithmic on(foiled grammar (ALCOGRAM). The ", "acronyms": [[132, 140]], "long-forms": [[101, 130]]}, {"text": "workbench (Hall et al, 2009). For SVM, we employ the radial basis function kernel (RBF) and we use the wrapper provided by Weka for", "acronyms": [[83, 86], [34, 37]], "long-forms": [[53, 74]]}, {"text": "This research has been supported in part by DARPA (under contract number FA8750-13-2-0005), NIH (NICHD award 1R01HD07532801), Keck Foundation (DT123107), NSF (IIS0835797), and", "acronyms": [[92, 95], [44, 49], [73, 75], [143, 145], [154, 157]], "long-forms": [[97, 102]]}, {"text": "ual resources on pairwise comparison task (Diff. = Difficulty lexicon, CF = Crowdflower) Features", "acronyms": [[71, 73]], "long-forms": [[76, 87]]}, {"text": "60  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 118?125, Seoul, South Korea, 5-6 July 2012.", "acronyms": [[100, 107]], "long-forms": [[50, 98]]}, {"text": " In Proceedings of the 17th International Conference on Computational Linguistics (COLING), Montreal, Canada.", "acronyms": [[83, 89]], "long-forms": [[56, 81]]}, {"text": "REF = obj123 SIZE = sizesensorreading85 SHAPE = shapesensorreading62 COLOUR = coloursensorreadning78", "acronyms": [[40, 45], [0, 3], [13, 17], [69, 75]], "long-forms": [[48, 68], [20, 39], [78, 100], [6, 12]]}, {"text": "In the table, P is precision; R is  recall; P&R is the harmonic mean of precision and  recall  (P&R = (2*P*R) / (P+R), corresponding to a  F-measure with a ?", "acronyms": [[96, 99], [44, 47]], "long-forms": [[103, 108], [19, 28], [36, 42]]}, {"text": "On Complexity of Word Order. Traitement Automatique des Langues (TAL), 41(1):273?300. ", "acronyms": [[65, 68]], "long-forms": [[29, 62]]}, {"text": "features: O-SEM ('ordinary semantics') and  I,'-SKEL (F-skeleton) of the type of a semantic ob-  ject, tile set-valued IS-CSTR (IS constraints) and  the binary MAX-F (for potential maximal focus).", "acronyms": [[119, 126], [10, 15], [44, 52], [160, 165]], "long-forms": [[128, 142], [18, 36], [54, 64]]}, {"text": " We discuss the available functions of the  prototype Chinese Sketch Engine (CSE) as well  as the robustness of language-independent ", "acronyms": [[77, 80]], "long-forms": [[54, 75]]}, {"text": "In the hierarchy of relations, Arguments (ARG) include Subject (SUBJ), Object (OBJ), Indirect Object (INDOBJ), Indirect Complement (IN-", "acronyms": [[64, 68], [79, 82], [42, 45], [102, 108]], "long-forms": [[55, 62], [71, 77], [31, 40], [85, 100]]}, {"text": "include other language skills such as listening and reading. These constitute the integrated (INT) items.", "acronyms": [[94, 97]], "long-forms": [[82, 92]]}, {"text": "In addition, for some nodes it is necessary to insist that adjunction is mandatory at  a node. In such a case, we say that the node has an Obligatory Adjoining (OA)  constraint.", "acronyms": [[161, 163]], "long-forms": [[139, 159]]}, {"text": "PP ??  ( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) ? ? ? ? ", "acronyms": [[9, 11], [0, 2], [31, 33], [50, 52]], "long-forms": [[14, 17]]}, {"text": "For Task 2-2, we design two kinds of evaluation metrics:  1) POS accuracy (POS-A)  This index is used to evaluate the performance ", "acronyms": [[75, 80]], "long-forms": [[61, 73]]}, {"text": " 1 I n t roduct ion   The development of Natural Language (NL) systems  for data retrieval has been a central issue in NL Pro- ", "acronyms": [[59, 61], [119, 121]], "long-forms": [[41, 57]]}, {"text": "The second and last step to generate qwn-ppv(s) consists of propagating over a WordNet graph to obtain a Personalized PageRanking Vector (PPV), one for each polarity.", "acronyms": [[138, 141], [37, 44]], "long-forms": [[105, 136]]}, {"text": "+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system", "acronyms": [[91, 93], [104, 106], [125, 127]], "long-forms": [[94, 102], [107, 123], [128, 143]]}]